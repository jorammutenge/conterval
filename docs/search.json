[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Conterval",
    "section": "",
    "text": "a data consulting company\n\n\n\n\n\nOur Services\n\n\nData Analysis\n\n\nData Cleaning\n\n\nData Engineering\n\n\nData Visualization\n\n\nAnalytics Engineering\n\n\nExtract Transform Load (ETL)\n\n\n\n\n\nContact Us\n\n\nüìß contact@conterval.com"
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "",
    "text": "In the last client project I worked on, I learned something about the group_by_dynamic function in Polars. While what I learned was surprising, the fact that I learned it during the project was not. This aligns with the philosophy of ‚Äúlet the work be the practice‚Äù that Cal Newport advocates, and I‚Äôm proud to say I follow it. Most people spend time learning about a particular technology before they use it in a project. Cal Newport‚Äôs philosophy suggests combining learning with doing. By practicing through actual work, you gain mastery. By the end of this project, my proficiency in Polars increased dramatically."
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#a-brief-overview-of-the-project",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#a-brief-overview-of-the-project",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "A brief overview of the project",
    "text": "A brief overview of the project\nThe client had forecast data for 2025 for electronic gadgets in monthly buckets, but she wanted it converted into weekly buckets. The dates in the data she presented all started at the beginning of each month. For instance, the first row contained the date January 1, 2025. Below is the forecast data from the client.\n\n\n\nshape: (96, 3)\n\n\n\nDate\nGadget\nForecast\n\n\ndate\nstr\ni16\n\n\n\n\n2025-01-01\n\"Headphones\"\n3439\n\n\n2025-01-01\n\"Keyboard\"\n1652\n\n\n2025-01-01\n\"Monitor\"\n311\n\n\n2025-01-01\n\"Mouse\"\n1139\n\n\n2025-01-01\n\"Printer\"\n123\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-12-01\n\"Mouse\"\n1385\n\n\n2025-12-01\n\"Printer\"\n166\n\n\n2025-12-01\n\"Smartwatch\"\n678\n\n\n2025-12-01\n\"Tablet\"\n496\n\n\n2025-12-01\n\"Webcam\"\n512"
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#how-we-solved-the-problem",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#how-we-solved-the-problem",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "How we solved the problem",
    "text": "How we solved the problem\nAt first, I thought I could solve the problem by simply dividing each forecast value by 7, the number of days in a week. However, a colleague quickly reminded me that not all months are created equal‚Äîsome have more days than others. So, I quickly abandoned that approach and searched for an alternative.\nThe solution that worked involved creating, from scratch, a single-column dataframe containing all the days of the year 2025, from January to December. We joined this dataframe with the client‚Äôs forecast data and then applied group_by_dynamic. It worked like a charm, but it also exposed something I hadn‚Äôt been fully aware of regarding group_by_dynamic."
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#creating-the-date-dataframe.",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#creating-the-date-dataframe.",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "Creating the date dataframe.",
    "text": "Creating the date dataframe.\nUsing datetime_range, I created a dataframe containing timeseries values for the entire year of 2025. The interval was set to 1 day, ensuring that every single date in the year is included. Below is the resulting timeseries dataframe. Then I extracted the month values from the dates to create a new column Month.\n\nfrom datetime import datetime\ndate_df = pl.DataFrame(\n    {\n        \"Date\": pl.datetime_range(\n            start=datetime(2025, 1, 1),\n            end=datetime(2025, 12, 31),\n            interval=\"1d\",\n            eager=True,\n        )\n    }\n).with_columns(pl.col('Date').dt.date(),\n               Month=pl.col('Date').dt.month())\ndate_df\n\n\nshape: (365, 2)\n\n\n\nDate\nMonth\n\n\ndate\ni8\n\n\n\n\n2025-01-01\n1\n\n\n2025-01-02\n1\n\n\n2025-01-03\n1\n\n\n2025-01-04\n1\n\n\n2025-01-05\n1\n\n\n‚Ä¶\n‚Ä¶\n\n\n2025-12-27\n12\n\n\n2025-12-28\n12\n\n\n2025-12-29\n12\n\n\n2025-12-30\n12\n\n\n2025-12-31\n12"
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#joining-the-two-dataframes",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#joining-the-two-dataframes",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "Joining the two dataframes",
    "text": "Joining the two dataframes\nBefore joining the two DataFrames, I converted the date values in the forecast dataset to month-only values. This ensured a unique common value between both dataframes, enabling the join to proceed. The dataframe below illustrates the transformation from date values to month-only values.\n\ndf = (data\n .with_columns(pl.col('Date').dt.month())\n )\ndf\n\n\nshape: (96, 3)\n\n\n\nDate\nGadget\nForecast\n\n\ni8\nstr\ni16\n\n\n\n\n1\n\"Headphones\"\n3439\n\n\n1\n\"Keyboard\"\n1652\n\n\n1\n\"Monitor\"\n311\n\n\n1\n\"Mouse\"\n1139\n\n\n1\n\"Printer\"\n123\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n12\n\"Mouse\"\n1385\n\n\n12\n\"Printer\"\n166\n\n\n12\n\"Smartwatch\"\n678\n\n\n12\n\"Tablet\"\n496\n\n\n12\n\"Webcam\"\n512\n\n\n\n\n\n\n\nThe two dataframes were joined using a left join. Also, I decided to divide the forecast values by 4 since most months have at least 4 weeks Here‚Äôs an example of the resulting dataframe for Headphones showing the monthly and weekly forecast.\n\n(date_df\n .join(df, left_on='Month', right_on='Date', how='left')\n .drop('Month')\n .with_columns(Weekly_Forecast=pl.col('Forecast').truediv(4).round(0).cast(pl.Int16))\n .sort('Date')\n .filter(pl.col('Gadget') == \"Headphones\")\n )\n\n\nshape: (365, 4)\n\n\n\nDate\nGadget\nForecast\nWeekly_Forecast\n\n\ndate\nstr\ni16\ni16\n\n\n\n\n2025-01-01\n\"Headphones\"\n3439\n860\n\n\n2025-01-02\n\"Headphones\"\n3439\n860\n\n\n2025-01-03\n\"Headphones\"\n3439\n860\n\n\n2025-01-04\n\"Headphones\"\n3439\n860\n\n\n2025-01-05\n\"Headphones\"\n3439\n860\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-12-27\n\"Headphones\"\n2992\n748\n\n\n2025-12-28\n\"Headphones\"\n2992\n748\n\n\n2025-12-29\n\"Headphones\"\n2992\n748\n\n\n2025-12-30\n\"Headphones\"\n2992\n748\n\n\n2025-12-31\n\"Headphones\"\n2992\n748"
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#converting-to-weekly-buckets",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#converting-to-weekly-buckets",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "Converting to weekly buckets",
    "text": "Converting to weekly buckets\nThe dates are still in days, but the client wants them in weeks so I used groub_by_dynamic with an interval of 7 days to convert them into weekly buckets. Below is the resulting dataframe.\n\nfrom great_tables import loc, style\n\nprocessed_df = (date_df\n .join(df, left_on='Month', right_on='Date', how='left')\n .drop('Month')\n .with_columns(Weekly_Forecast=pl.col('Forecast').truediv(4).round(0).cast(pl.Int16))\n .sort('Date')\n .filter(pl.col('Gadget') == \"Headphones\")\n .group_by_dynamic('Date', every='7d')\n .agg(pl.first('Gadget', 'Forecast', 'Weekly_Forecast'))\n )\n\nprocessed_df[0:10].style.tab_style(\n    style.fill(\"yellow\"),\n    loc.body(\n        rows=pl.col(\"Date\").dt.year() == 2024,\n    ),\n)\n\n\n\n\n\n\n\nDate\nGadget\nForecast\nWeekly_Forecast\n\n\n\n\n2024-12-26\nHeadphones\n3439\n860\n\n\n2025-01-02\nHeadphones\n3439\n860\n\n\n2025-01-09\nHeadphones\n3439\n860\n\n\n2025-01-16\nHeadphones\n3439\n860\n\n\n2025-01-23\nHeadphones\n3439\n860\n\n\n2025-01-30\nHeadphones\n3439\n860\n\n\n2025-02-06\nHeadphones\n2620\n655\n\n\n2025-02-13\nHeadphones\n2620\n655\n\n\n2025-02-20\nHeadphones\n2620\n655\n\n\n2025-02-27\nHeadphones\n2620\n655\n\n\n\n\n\n\n        \n\n\n\nDo you notice the peculiarity that group_by_dynamic introduces? None of our original DataFrames contained the year 2024, yet after using group_by_dynamic, we now see 2024. What‚Äôs going on here? I was initially unaware of this behavior. It turns out that group_by_dynamic shifts date values by the interval specified in the every parameter. Since we used a 7-day interval, the date values were moved 7 days back, causing the appearance of 2024.\nBut wait‚Äîthe client specifically needs forecast data for 2025. How can we address this? Thankfully, the developers of Polars anticipated this issue and provided a solution. As outlined in the documentation, adding the start_by parameter with the value \"datapoint\" to group_by_dynamic resolves the problem. With this adjustment, the year 2024 disappeared entirely.\n\n(date_df\n .join(df, left_on='Month', right_on='Date', how='left')\n .drop('Month')\n .filter(pl.col('Gadget') == 'Monitor')\n .with_columns(Weekly_Forecast=pl.col('Forecast').truediv(4).round(0).cast(pl.Int16))\n .sort('Date')\n .group_by_dynamic('Date', every='7d', start_by='datapoint')\n .agg(pl.first('Gadget', 'Forecast', 'Weekly_Forecast'))\n )\n\n\nshape: (53, 4)\n\n\n\nDate\nGadget\nForecast\nWeekly_Forecast\n\n\ndate\nstr\ni16\ni16\n\n\n\n\n2025-01-01\n\"Monitor\"\n311\n78\n\n\n2025-01-08\n\"Monitor\"\n311\n78\n\n\n2025-01-15\n\"Monitor\"\n311\n78\n\n\n2025-01-22\n\"Monitor\"\n311\n78\n\n\n2025-01-29\n\"Monitor\"\n311\n78\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-12-03\n\"Monitor\"\n325\n81\n\n\n2025-12-10\n\"Monitor\"\n325\n81\n\n\n2025-12-17\n\"Monitor\"\n325\n81\n\n\n2025-12-24\n\"Monitor\"\n325\n81\n\n\n2025-12-31\n\"Monitor\"\n325\n81\n\n\n\n\n\n\n\nWith the date issue resolved, we can now proceed to develop the code needed to create the final dataset for presentation to the client.\n\ngadget_list = df['Gadget'].unique().to_list()\n\nbucket_dfs = []\nfor gadget in gadget_list:\n    bucket_df = (date_df\n        .join(df, left_on='Month', right_on='Date', how='left')\n        .drop('Month')\n        .filter(pl.col('Gadget') == gadget)\n        .with_columns(Weekly_Forecast=pl.col('Forecast').truediv(4).round(0).cast(pl.Int16))\n        .sort('Date')\n        .group_by_dynamic('Date', every='7d', start_by='datapoint')\n        .agg(pl.first('Gadget', 'Forecast', 'Weekly_Forecast'))\n        )\n    bucket_dfs.append(bucket_df)\n\nall_bucket_df = pl.concat(bucket_dfs).drop('Forecast')\nall_bucket_df\n\n\nshape: (424, 3)\n\n\n\nDate\nGadget\nWeekly_Forecast\n\n\ndate\nstr\ni16\n\n\n\n\n2025-01-01\n\"Webcam\"\n108\n\n\n2025-01-08\n\"Webcam\"\n108\n\n\n2025-01-15\n\"Webcam\"\n108\n\n\n2025-01-22\n\"Webcam\"\n108\n\n\n2025-01-29\n\"Webcam\"\n108\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-12-03\n\"Keyboard\"\n346\n\n\n2025-12-10\n\"Keyboard\"\n346\n\n\n2025-12-17\n\"Keyboard\"\n346\n\n\n2025-12-24\n\"Keyboard\"\n346\n\n\n2025-12-31\n\"Keyboard\"\n346\n\n\n\n\n\n\n\nThe lesson to remember is that group_by_dynamic will move the date values back by the specified interval you set in the every parameter. If you want to maintain the date values in your dataset, you must add ananother parameter start_at and set it to \"datapoint\".\nReach out if you need help with your data problems. Also, take a look at our Polars course to improve your data analysis skills using this fast Python library."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Conterval",
    "section": "",
    "text": "Tranforming timeseries data with group by and group by dynamic in polars\n\n\n\n\n\n\n\n\n\n\n\n2025-02-03\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I learned about group by dynamic in polars while working on a client‚Äôs project\n\n\n\n\n\n\n\n\n\n\n\n2024-12-30\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create charts from The Economist magazine using plotly\n\n\n\n\n\n\n\n\n\n\n\n2024-12-15\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nWhat tool should you use as a data analyst?\n\n\n\n\n\n\n\n\n\n\n\n2024-12-01\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nHow we helped a bakery generate forecast by bread type using polars\n\n\n\n\n\n\n\n\n\n\n\n2024-11-18\n\n\nJoram Mutenge\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "",
    "text": "A few weeks ago, Conterval did a consulting gig for a medium-sized bakery. This bakery makes white and brown bread, which it sells to a major retail store here in the USA. The bakery contacted our company to help clean up their forecast data and generate a forecast for each bread type.\nThe gig turned out to be an interesting experience, so we asked the bakery if I could write about the experience on the company blog, and they said yes. In this post, I‚Äôll share what the bakery‚Äôs problem was and the solution we devised to solve it."
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#problem",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#problem",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Problem",
    "text": "Problem\nThe bakery receives an Excel file with forecast data from a major US retail store every week. This file contains 2 columns: Date (the 1st of every month from January to December) and Forecast (the number of loaves of bread they want in that month).\nThe challenge was that the retail store did not provide a separate forecast value for white and brown bread. The retail store just provided a single forecast value. It was up to the bakery to divide that number into how many loaves of white or brown bread to make. It turns out this was a challenging task."
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#solution",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#solution",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Solution",
    "text": "Solution\nCreate a systematic process that determines how many loaves of bread should be made for each bread type based on the provided forecast value for that month. This information should be presented in an easy to understand table.\n\n\n\n\n\n\nNote\n\n\n\nThe generated table should be easy to update based on the new forecast data provided by the retail store."
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#dataset",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#dataset",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Dataset",
    "text": "Dataset\nWe‚Äôll not use the actual data from the bakery, rather we‚Äôll use fictional data to demonstrate the solution. Here‚Äôs the baker‚Äôs sales data from last year.\n\n\n\nshape: (24, 3)\n\n\n\nDate\nBread\nSales\n\n\ndate\nstr\ni64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\n\n\n2023-01-01\n\"White\"\n203\n\n\n2023-02-01\n\"Brown\"\n329\n\n\n2023-02-01\n\"White\"\n304\n\n\n2023-03-01\n\"Brown\"\n201\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2023-10-01\n\"White\"\n425\n\n\n2023-11-01\n\"Brown\"\n383\n\n\n2023-11-01\n\"White\"\n297\n\n\n2023-12-01\n\"Brown\"\n248\n\n\n2023-12-01\n\"White\"\n200\n\n\n\n\n\n\n\nHere‚Äôs the forecast data from the retail store.\n\n\n\nshape: (12, 2)\n\n\n\nDate\nForecast\n\n\ndate\ni64\n\n\n\n\n2024-01-01\n897\n\n\n2024-02-01\n945\n\n\n2024-03-01\n865\n\n\n2024-04-01\n754\n\n\n2024-05-01\n1010\n\n\n‚Ä¶\n‚Ä¶\n\n\n2024-08-01\n777\n\n\n2024-09-01\n922\n\n\n2024-10-01\n848\n\n\n2024-11-01\n1002\n\n\n2024-12-01\n831"
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#the-math",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#the-math",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "The math",
    "text": "The math\nDetermining how many brown or white loaves of bread to make was not as easy as dividing the forecast value by 2. Why? Because in some months, the retail store buys more white bread than brown bread. In other months, it‚Äôs the reverse.\nWe decided to leverage some timeseries calculations by doing a rolling sum with a 3-months window of last year‚Äôs sales by bread type. The idea was to get the weight or percentage for each bread type and use that to determine the number of loaves to make from the forecast value.\nThis math is quite involving, but it‚Äôs easy to follow along with the data."
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#implementing-the-math",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#implementing-the-math",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Implementing the math",
    "text": "Implementing the math\nHere are the formulas for calculating the rolling sum for the rows of each bread type.\n\\[\\text{Row 1} = \\text{Jan} + \\text{Feb} + \\text{Mar}\\]\n\\[\\text{Row 2} = \\text{Feb} + \\text{Mar} + \\text{Apr}\\]\n\\[\\text{...}\\]\n\\[\\text{Second Last Row} = \\text{Nov} + \\text{Dec}\\]\n\\[\\text{Last Row} = \\text{Dec}\\]\nAnd here is a visualization showing the calculated rolling sum values for each row of brown bread.\n\n\n\n\n\ngraph TD\n    classDef sumStyle fill:#FFE4B5,stroke:#333,stroke-width:2px;\n\n    sum1[\"342 + 329 + 201 = 872\"]:::sumStyle --&gt; sum2[\"329 + 201 + 203 = 733\"]:::sumStyle\n    sum2 --&gt; sum3[\"201 + 203 + 300 = 704\"]:::sumStyle\n    sum3 --&gt; sum4[\"203 + 300 + 473 = 976\"]:::sumStyle\n    sum4 --&gt; sum5[\"300 + 473 + 287 = 1060\"]:::sumStyle\n    sum5 --&gt; sum6[\"473 + 287 + 446 = 1206\"]:::sumStyle\n    sum6 --&gt; sum7[\"287 + 446 + 305 = 1038\"]:::sumStyle\n    sum7 --&gt; sum8[\"446 + 305 + 253 = 1004\"]:::sumStyle\n    sum8 --&gt; sum9[\"305 + 253 + 383 = 941\"]:::sumStyle\n    sum9 --&gt; sum10[\"253 + 383 + 248 = 884\"]:::sumStyle\n    sum10 --&gt; sum11[\"383 + 248 = 631\"]:::sumStyle\n    sum11 --&gt; sum12[\"248\"]:::sumStyle\n\n\n\n\n\n\nNow that we know what values to expect, we can implement the math. Initially, we thought that doing a rolling_sum polars function would perform the calculation shown above but it didn‚Äôt, at least not entirely.\nLet‚Äôs demonstrate this calculation to see where it fell short.\n\n(sales_df\n .filter(pl.col('Bread').eq('Brown'))\n .with_columns(Rol_3month=pl.col('Sales').rolling_sum(window_size=3))\n )\n\n\nshape: (12, 4)\n\n\n\nDate\nBread\nSales\nRol_3month\n\n\ndate\nstr\ni64\ni64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\nnull\n\n\n2023-02-01\n\"Brown\"\n329\nnull\n\n\n2023-03-01\n\"Brown\"\n201\n872\n\n\n2023-04-01\n\"Brown\"\n203\n733\n\n\n2023-05-01\n\"Brown\"\n300\n704\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2023-08-01\n\"Brown\"\n446\n1206\n\n\n2023-09-01\n\"Brown\"\n305\n1038\n\n\n2023-10-01\n\"Brown\"\n253\n1004\n\n\n2023-11-01\n\"Brown\"\n383\n941\n\n\n2023-12-01\n\"Brown\"\n248\n884\n\n\n\n\n\n\n This gives us some of the values we want, but it creates null values for the first 2 rows. To rectify the null value problem on the first two rows, we shifted the values in Rol_3month up by 2 rows.\n\n(sales_df\n .filter(pl.col('Bread').eq('Brown'))\n .with_columns(Rol_3month=pl.col('Sales').rolling_sum(window_size=3))\n .with_columns(Rol_3month_Shift=pl.col('Sales').rolling_sum(window_size=3).shift(-2))\n )\n\n\nshape: (12, 5)\n\n\n\nDate\nBread\nSales\nRol_3month\nRol_3month_Shift\n\n\ndate\nstr\ni64\ni64\ni64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\nnull\n872\n\n\n2023-02-01\n\"Brown\"\n329\nnull\n733\n\n\n2023-03-01\n\"Brown\"\n201\n872\n704\n\n\n2023-04-01\n\"Brown\"\n203\n733\n976\n\n\n2023-05-01\n\"Brown\"\n300\n704\n1060\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2023-08-01\n\"Brown\"\n446\n1206\n1004\n\n\n2023-09-01\n\"Brown\"\n305\n1038\n941\n\n\n2023-10-01\n\"Brown\"\n253\n1004\n884\n\n\n2023-11-01\n\"Brown\"\n383\n941\nnull\n\n\n2023-12-01\n\"Brown\"\n248\n884\nnull\n\n\n\n\n\n\n The problem of null values in the first 2 rows is solved, but another problem is created. The last 2 rows now have null values. At this point, we knew that rolling_sum wasn‚Äôt going to work.\nThe reason why rolling sum didn‚Äôt work is that on the first row, we don‚Äôt yet have 3 values to add so the sum is null, the same applies to the second row. But on the second row, we have 3 values in the window to add that‚Äôs why the first value shows up on row 3.\nBut since we wanted the value on row 3 to be on the first row, we shifted the values up by 2 rows, but that only created null values on the bottom two rows. Also, since the rolling sum shifts down one row to get the next 3 values, eventually there won‚Äôt be enough 3 values to add. That‚Äôs why we have null values.\nHowever, from the formulas above, we see that if there are no 3 values to add, the rolling sum calculation proceeds by calculating the available values. So for the second to last row, it‚Äôs only 2 values (Nov + Dec), and for the last row, it‚Äôs only 1 value (Dec)."
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#successful-implementation-of-solution",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#successful-implementation-of-solution",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Successful implementation of solution",
    "text": "Successful implementation of solution\nThe solution that worked involved the use of implode. Here‚Äôs how we implemented it. Let‚Äôs focus on brown bread only so we can see that the rolling sum values in the dataframe are the same as those in the visualization.\nWe‚Äôll begin by creating a list of all the dates in the sales data.\n\ndate_list = sales_df['Date'].unique().to_list()\ndate_list\n\n[datetime.date(2023, 1, 1),\n datetime.date(2023, 2, 1),\n datetime.date(2023, 3, 1),\n datetime.date(2023, 4, 1),\n datetime.date(2023, 5, 1),\n datetime.date(2023, 6, 1),\n datetime.date(2023, 7, 1),\n datetime.date(2023, 8, 1),\n datetime.date(2023, 9, 1),\n datetime.date(2023, 10, 1),\n datetime.date(2023, 11, 1),\n datetime.date(2023, 12, 1)]\n\n\n Now, let‚Äôs write some code to calculate the Rol_3month value for the first date in date_list. This date value will be accessed with 0 index as in date_list[0].\n\n(sales_df\n .filter(pl.col('Bread').eq('Brown'))\n .with_columns(Sales_List=pl.col('Sales').implode())\n .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n .filter(pl.col('Date').eq(date_list[0]))\n)\n\n\nshape: (1, 5)\n\n\n\nDate\nBread\nSales\nSales_List\nRol_3month\n\n\ndate\nstr\ni64\nlist[i64]\ni64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\n[342, 329, ‚Ä¶ 248]\n872\n\n\n\n\n\n\n In the code above, we filtered the data to only show brown bread, then created a column Sales_List using implode. This stores all sales values from January to December into a single list. To calculate the Rol_3month, we slice the list of sales values to only select the first available 3 values and then add them up. Finally, we only get the row in the dataframe that corresponds to the chosen date, which is the first date in date_list.\nLet‚Äôs reuse this code to calculate the Rol_3month value for white bread. Because we want to have a single dataframe showing the results for brown and white bread, we‚Äôll use vstack to vertically combine the dataframes.\n\n(sales_df\n .filter(pl.col('Bread').eq('Brown'))\n .with_columns(Sales_List=pl.col('Sales').implode())\n .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n .filter(pl.col('Date').eq(date_list[0]))\n .vstack(sales_df\n .filter(pl.col('Bread').eq('White'))\n .with_columns(Sales_List=pl.col('Sales').implode())\n .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n .filter(pl.col('Date').eq(date_list[0])))\n)\n\n\nshape: (2, 5)\n\n\n\nDate\nBread\nSales\nSales_List\nRol_3month\n\n\ndate\nstr\ni64\nlist[i64]\ni64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\n[342, 329, ‚Ä¶ 248]\n872\n\n\n2023-01-01\n\"White\"\n203\n[203, 304, ‚Ä¶ 200]\n880"
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#calculating-percentages",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#calculating-percentages",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Calculating percentages",
    "text": "Calculating percentages\nHere‚Äôs the formula we used to calculate the percentage or weight for each bread type. Let‚Äôs focus on brown bread for the month of January.\n\\[\\% \\text{ of Brown Bread} = \\frac{\\text{January Rol\\_3month}}{\\text{January Rol\\_3month} + \\text{White Bread January Rol\\_3month}}\\]\nWe are dividing each Rol_3month value for every bread type by the sum of the Rol_3month values for both bread types. Let‚Äôs put this into code. Also, we don‚Äôt need Sales_List, so we‚Äôll drop it.\n\n(sales_df\n .filter(pl.col('Bread').eq('Brown'))\n .with_columns(Sales_List=pl.col('Sales').implode())\n .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n .filter(pl.col('Date').eq(date_list[0]))\n .drop('Sales_List')\n .vstack(sales_df\n .filter(pl.col('Bread').eq('White'))\n .with_columns(Sales_List=pl.col('Sales').implode())\n .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n .filter(pl.col('Date').eq(date_list[0]))\n .drop('Sales_List'))\n .with_columns(Percentage=pl.col('Rol_3month') / pl.col('Rol_3month').sum())\n )\n\n\nshape: (2, 5)\n\n\n\nDate\nBread\nSales\nRol_3month\nPercentage\n\n\ndate\nstr\ni64\ni64\nf64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\n872\n0.497717\n\n\n2023-01-01\n\"White\"\n203\n880\n0.502283\n\n\n\n\n\n\n\nThese are the values we want, but we‚Äôve only calculated for the first date in date_list. We have to perform this calculation for every date in date_list. Rather than doing it manually 12 times, we‚Äôll use a for loop to loop through the date_list.\nIt turns out that looping doesn‚Äôt work on a list of dates, so we‚Äôll create a list of 12 numbers from 0 to 11. These numbers will be used as indices to represent each date item in date_list. Thus, to use the first date in the list, we use date_list[0].\nLet‚Äôs create a list of the numbers and store them in a variable called num_list.\n\nnum_list = list(range(len(date_list)))\nnum_list\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\n\nNow let‚Äôs implement the for loop to get the desired dataframe.\n\npct_dfs = []\nfor i in num_list:\n    pct_df = (sales_df\n              .filter(pl.col('Bread').eq('Brown'))\n              .with_columns(Sales_List=pl.col('Sales').implode())\n              .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n              .filter(pl.col('Date').eq(date_list[i]))\n              .drop('Sales_List')\n              .vstack(sales_df\n              .filter(pl.col('Bread').eq('White'))\n              .with_columns(Sales_List=pl.col('Sales').implode())\n              .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n              .filter(pl.col('Date').eq(date_list[i]))\n              .drop('Sales_List'))\n              .with_columns(Percentage=pl.col('Rol_3month') / pl.col('Rol_3month').sum())\n              )\n    pct_dfs.append(pct_df)\ndf_with_pct = pl.concat(pct_dfs)\ndf_with_pct\n\n\nshape: (24, 5)\n\n\n\nDate\nBread\nSales\nRol_3month\nPercentage\n\n\ndate\nstr\ni64\ni64\nf64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\n872\n0.497717\n\n\n2023-01-01\n\"White\"\n203\n880\n0.502283\n\n\n2023-02-01\n\"Brown\"\n329\n872\n0.497717\n\n\n2023-02-01\n\"White\"\n304\n880\n0.502283\n\n\n2023-03-01\n\"Brown\"\n201\n872\n0.497717\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2023-10-01\n\"White\"\n425\n880\n0.502283\n\n\n2023-11-01\n\"Brown\"\n383\n872\n0.497717\n\n\n2023-11-01\n\"White\"\n297\n880\n0.502283\n\n\n2023-12-01\n\"Brown\"\n248\n872\n0.497717\n\n\n2023-12-01\n\"White\"\n200\n880\n0.502283"
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#joining-forecast-data",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#joining-forecast-data",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Joining forecast data",
    "text": "Joining forecast data\nTo get the forecast values for each bread type based on percentage values, we must join our dataframe with the forecast data provided by the retail store. We‚Äôll join the dataframes on Date column.\n\n\n\n\n\n\nNote\n\n\n\nOur sales data has the year 2023 while the forecast data has the year 2024. This means we won‚Äôt be able to join. We have to modify the dates so they match.\n\n\nTo make the dates in both dataframes match, we‚Äôll remove the year in the date value. Below is the code that removes the year in the date value for the sales data.\n\n(df_with_pct\n .with_columns(pl.col('Date').dt.strftime('%m-%d'))\n)\n\n\nshape: (24, 5)\n\n\n\nDate\nBread\nSales\nRol_3month\nPercentage\n\n\nstr\nstr\ni64\ni64\nf64\n\n\n\n\n\"01-01\"\n\"Brown\"\n342\n872\n0.497717\n\n\n\"01-01\"\n\"White\"\n203\n880\n0.502283\n\n\n\"02-01\"\n\"Brown\"\n329\n872\n0.497717\n\n\n\"02-01\"\n\"White\"\n304\n880\n0.502283\n\n\n\"03-01\"\n\"Brown\"\n201\n872\n0.497717\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"10-01\"\n\"White\"\n425\n880\n0.502283\n\n\n\"11-01\"\n\"Brown\"\n383\n872\n0.497717\n\n\n\"11-01\"\n\"White\"\n297\n880\n0.502283\n\n\n\"12-01\"\n\"Brown\"\n248\n872\n0.497717\n\n\n\"12-01\"\n\"White\"\n200\n880\n0.502283\n\n\n\n\n\n\n\nIn the code below, we remove the year in the forecast data and join the two dataframes in a single dataframe called combined_df.\n\ncombined_df = (df_with_pct\n .with_columns(pl.col('Date').dt.strftime('%m-%d'))\n .join(forecast_df.with_columns(pl.col('Date').dt.strftime('%m-%d')),\n       on='Date', how='left')\n )\ncombined_df\n\n\nshape: (24, 6)\n\n\n\nDate\nBread\nSales\nRol_3month\nPercentage\nForecast\n\n\nstr\nstr\ni64\ni64\nf64\ni64\n\n\n\n\n\"01-01\"\n\"Brown\"\n342\n872\n0.497717\n897\n\n\n\"01-01\"\n\"White\"\n203\n880\n0.502283\n897\n\n\n\"02-01\"\n\"Brown\"\n329\n872\n0.497717\n945\n\n\n\"02-01\"\n\"White\"\n304\n880\n0.502283\n945\n\n\n\"03-01\"\n\"Brown\"\n201\n872\n0.497717\n865\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"10-01\"\n\"White\"\n425\n880\n0.502283\n848\n\n\n\"11-01\"\n\"Brown\"\n383\n872\n0.497717\n1002\n\n\n\"11-01\"\n\"White\"\n297\n880\n0.502283\n1002\n\n\n\"12-01\"\n\"Brown\"\n248\n872\n0.497717\n831\n\n\n\"12-01\"\n\"White\"\n200\n880\n0.502283\n831"
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#calculating-new-forecast",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#calculating-new-forecast",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Calculating new forecast",
    "text": "Calculating new forecast\nNow we have all the data needed to calculate the forecast for each bread type. All the forecast values are rounded to the nearest whole number. After all, you cannot make 1.67 loaves of bread!\n\nnew_fcst_df = (combined_df\n .with_columns(New_Forecast=(pl.col('Percentage') * pl.col('Forecast')).round().cast(pl.Int16))\n .with_columns(pl.col('Date').add(pl.lit('-2024')).str.strptime(pl.Date, \"%m-%d-%Y\"))\n )\nnew_fcst_df\n\n\nshape: (24, 7)\n\n\n\nDate\nBread\nSales\nRol_3month\nPercentage\nForecast\nNew_Forecast\n\n\ndate\nstr\ni64\ni64\nf64\ni64\ni16\n\n\n\n\n2024-01-01\n\"Brown\"\n342\n872\n0.497717\n897\n446\n\n\n2024-01-01\n\"White\"\n203\n880\n0.502283\n897\n451\n\n\n2024-02-01\n\"Brown\"\n329\n872\n0.497717\n945\n470\n\n\n2024-02-01\n\"White\"\n304\n880\n0.502283\n945\n475\n\n\n2024-03-01\n\"Brown\"\n201\n872\n0.497717\n865\n431\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2024-10-01\n\"White\"\n425\n880\n0.502283\n848\n426\n\n\n2024-11-01\n\"Brown\"\n383\n872\n0.497717\n1002\n499\n\n\n2024-11-01\n\"White\"\n297\n880\n0.502283\n1002\n503\n\n\n2024-12-01\n\"Brown\"\n248\n872\n0.497717\n831\n414\n\n\n2024-12-01\n\"White\"\n200\n880\n0.502283\n831\n417"
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#final-output",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#final-output",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Final output",
    "text": "Final output\nWe now have the forecast values for each bread type, but we must present the data in a format that is not only human-readable but also easy to understand. We‚Äôll select the relevant columns and transform the data into the desired format.\n\ntable_df = (new_fcst_df\n .select('Date','Bread','New_Forecast')\n .with_columns(pl.col('Date').dt.strftime('%b'))\n .pivot(on='Date', index='Bread')\n )\ntable_df\n\n\nshape: (2, 13)\n\n\n\nBread\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nstr\ni16\ni16\ni16\ni16\ni16\ni16\ni16\ni16\ni16\ni16\ni16\ni16\n\n\n\n\n\"Brown\"\n446\n470\n431\n375\n503\n373\n478\n387\n459\n422\n499\n414\n\n\n\"White\"\n451\n475\n434\n379\n507\n377\n482\n390\n463\n426\n503\n417\n\n\n\n\n\n\n\nThis format is better, but since at Conterval we‚Äôre sticklers for aesthetics, we decided to make the final forecast output look even better. To do this, we used a library called great-tables.\n\nfrom great_tables import GT, style, loc, google_font, html\n\nmonth_list = table_df.columns[1:]\ncol_spacing = {month: '60px' for month in month_list}\n\n(\n    GT(table_df, rowname_col=\"Bread\")\n    .tab_stubhead(label=html('&lt;b&gt;Bread'))\n    .tab_header(title=html(\"&lt;h2&gt;Bread Types Forecast 2024&lt;/h2&gt;\"))\n    .tab_options(\n        table_background_color='#ffbe6f',\n        row_group_font_weight='bold'\n    )\n    .tab_style(\n        style=style.text(weight='bold', font=google_font(name=\"Fjalla One\")),\n        locations=loc.column_header()\n    )\n    .cols_width(cases=col_spacing)\n)\n\n\n\n\n\n\n\nBread Types Forecast 2024\n\n\n\nBread\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\n\n\nBrown\n446\n470\n431\n375\n503\n373\n478\n387\n459\n422\n499\n414\n\n\nWhite\n451\n475\n434\n379\n507\n377\n482\n390\n463\n426\n503\n417\n\n\n\n\n\n\n        \n\n\n\nContact us for help with your data problems. Also check out our Polars course to level up your data analysis skills with this fast Python library."
  },
  {
    "objectID": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html",
    "href": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html",
    "title": "How to create charts from The Economist magazine using plotly",
    "section": "",
    "text": "We at Conterval have always been fans of the charts from The Economist magazine. No publication does a better job of creating static visualizations you can use in print. We love their charts because they‚Äôre simple, yet they manage to convey the relevant message contained in the data. More importantly, they adhere to Ben Shneiderman‚Äôs point that:\n\nThe purpose of visualization is insight, not pictures.\n\nIt turns out that having too much graphic detail on your charts is not as effective at conveying the message as having a simple chart. The Economist is the master of simple, yet informative charts.\nUnfortunately, many people focus on the chart‚Äôs aesthetics‚Äîhow it looks to the human eye. In so doing, they neglect the message contained in the data, which they are supposed to convey to the audience.\nAlthough we may not know the specific software or graphing library used by The Economist for their charts, their simplicity makes them easy to recreate with any competent graphing tool. In this case, we‚Äôll recreate two of their charts using the Python graphing library Plotly."
  },
  {
    "objectID": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#introduction",
    "href": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#introduction",
    "title": "How to create charts from The Economist magazine using plotly",
    "section": "",
    "text": "We at Conterval have always been fans of the charts from The Economist magazine. No publication does a better job of creating static visualizations you can use in print. We love their charts because they‚Äôre simple, yet they manage to convey the relevant message contained in the data. More importantly, they adhere to Ben Shneiderman‚Äôs point that:\n\nThe purpose of visualization is insight, not pictures.\n\nIt turns out that having too much graphic detail on your charts is not as effective at conveying the message as having a simple chart. The Economist is the master of simple, yet informative charts.\nUnfortunately, many people focus on the chart‚Äôs aesthetics‚Äîhow it looks to the human eye. In so doing, they neglect the message contained in the data, which they are supposed to convey to the audience.\nAlthough we may not know the specific software or graphing library used by The Economist for their charts, their simplicity makes them easy to recreate with any competent graphing tool. In this case, we‚Äôll recreate two of their charts using the Python graphing library Plotly."
  },
  {
    "objectID": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#dumbbell-plot",
    "href": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#dumbbell-plot",
    "title": "How to create charts from The Economist magazine using plotly",
    "section": "Dumbbell plot",
    "text": "Dumbbell plot\nA dumbbell plot is a dot plot that uses straight lines to connect two points for each group. This type of chart is ideal for illustrating changes in a variable over two time points or highlighting the range of a variable across multiple groups. The Economist dumbbell chart below has two groups (1970 and 2020) joined by two dots. These years are points in time.\n\n\n\n\nThe Economist dumbbell chart\n\n\n\n\nSuits dataset\nSince we didn‚Äôt have the data used to create the chart above, we put together some fake department data about a company from one of our favorite TV shows, Suits.\nBelow is a dataframe showing customer satisfaction scores for each department of the fictional firm Pearson Specter Litt for the years 2023 and 2024.\n\n\n\nshape: (8, 3)\n\n\n\nDepartment\n2023\n2024\n\n\nstr\ni64\ni64\n\n\n\n\n\"Customer Service\"\n80\n70\n\n\n\"Finance\"\n60\n70\n\n\n\"HR\"\n75\n80\n\n\n\"IT\"\n45\n55\n\n\n\"Operations\"\n85\n78\n\n\n\"Procurement\"\n32\n55\n\n\n\"Production\"\n20\n35\n\n\n\"Sales\"\n60\n65\n\n\n\n\n\n\n\nAnd now here‚Äôs a Plotly recreation of our dumbbell plot.\n\nBG_COLOR = '#E9EDF0'\nRED = '#E3120B'\nGREY = '#5e5c64'\n\nfig = go.Figure()\n\nfor i in range(df.shape[0]):\n    fig.add_trace(go.Scatter(\n        x=[df[\"2023\"][i], df[\"2024\"][i]],\n        y=[df[\"Department\"][i], df[\"Department\"][i]],\n        mode='lines+markers',\n        line=dict(color='#598080', width=4),  # Thicker line\n        marker=dict(size=16),  # Bigger dots\n        showlegend=False\n    ))\n    fig.add_trace(go.Scatter(\n        x=[df[\"2023\"][i]],\n        y=[df[\"Department\"][i]],\n        mode='markers',\n        marker=dict(color=GREY, size=16),  # Bigger dots\n        showlegend=False\n    ))\n    fig.add_trace(go.Scatter(\n        x=[df[\"2024\"][i]],\n        y=[df[\"Department\"][i]],\n        mode='markers',\n        marker=dict(color=RED, size=16),  # Bigger dots\n        showlegend=False\n    ))\n\n# Update layout to customize appearance\nfig.update_layout(\n    title=\"&lt;b&gt;Engagement score declined in&lt;br&gt;customer service & operations\",\n    title_font=dict(size=26),\n    title_y=.9,\n    plot_bgcolor=BG_COLOR,\n    paper_bgcolor=BG_COLOR,\n    height=600,\n    margin=dict(t=180, b=80),\n    xaxis=dict(\n        side=\"top\",  # Move x-axis to the top\n        tickfont=dict(size=18, family=\"Inter\")  # Increase the font size of the x-axis ticks\n    ),\n    yaxis=dict(\n        tickfont=dict(size=16, family=\"Inter\")\n    ),\n    shapes=[\n        dict(\n            type=\"line\",\n            xref=\"paper\",\n            yref=\"paper\",\n            x0=-0.12, y0=1.5,  \n            x1=0.022, y1=1.5,  \n            line=dict(\n                color=RED,  \n                width=10  \n            )\n        )],\n    font=dict(family=\"Inter\")  # Set the global font to \"Inter\"\n)\n\nfig.add_annotation(\n    dict(\n        text=\"&lt;b&gt;2023\",\n        x=0.1,  # x position (0 means far left)\n        y=0.72,  # y position (adjust as necessary)\n        xref=\"paper\",\n        yref=\"paper\",\n        showarrow=False,  # No arrow\n        font=dict(\n            family=\"Inter\",  # Font family\n            size=22,  # Font size\n            color=GREY  # Font color\n        ),\n        align=\"left\"\n    ),\n)\n\nfig.add_annotation(\n    dict(\n        text=\"&lt;b&gt;2024\",\n        x=0.6,  # x position (0 means far left)\n        y=0.72,  # y position (adjust as necessary)\n        xref=\"paper\",\n        yref=\"paper\",\n        showarrow=False,  # No arrow\n        font=dict(\n            family=\"Inter\",  # Font family\n            size=22,  # Font size\n            color=RED  # Font color\n        ),\n        align=\"left\"\n    ),\n)\n\nfig.add_layout_image(\n    dict(\n        source=f\"data:image/png;base64,{encoded_image}\",\n        xref=\"paper\",\n        yref=\"paper\",\n        x=.94,\n        y=-0.18,\n        xanchor=\"right\",\n        yanchor=\"bottom\",\n        sizex=0.22,\n        sizey=0.22,\n        layer=\"above\"\n    )\n)\n\nfig.add_annotation(\n    dict(\n        text=\"Source: Pearson Specter Litt\",\n        x=-0.14,  # x position (0 means far left)\n        y=-0.175,  # y position (adjust as necessary)\n        xref=\"paper\",\n        yref=\"paper\",\n        showarrow=False,  # No arrow\n        font=dict(\n            family=\"Inter\",  # Font family\n            size=14,  # Font size\n            color=GREY  # Font color\n        ),\n        align=\"left\"\n    ),\n)\n\nfig.show()\n\n                                                \n\n\n\nDisregard the differences between the data in my chart and The Economist‚Äôs chart, and focus on the style. You‚Äôll see that the styles are identical. This highlights Plotly‚Äôs versatility as a graphing library‚Äîallowing you to fully customize any chart to match your preferences."
  },
  {
    "objectID": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#stacked-bar-plot",
    "href": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#stacked-bar-plot",
    "title": "How to create charts from The Economist magazine using plotly",
    "section": "Stacked bar plot",
    "text": "Stacked bar plot\nA stacked bar chart is a type of bar graph where each bar is divided into segments that represent different subcategories. It shows how each subcategory contributes to the total value of a larger category, making it easy to see the breakdown of the whole.\n\n\n\n\nThe Economist stacked bar chart\n\n\n\nWhat makes this chart unique is the placement of the major group names above each bar, rather than beside them. This design choice offers two key advantages: it makes it easier to associate each category with its corresponding bar and saves space, resulting in a more compact plot. Additionally, the data varies significantly‚Äîfor example, from 11 to 998‚Äîwhich would distort the scale in a regular bar chart. By opting for a stacked bar chart, The Economist effectively addressed this issue, demonstrating thoughtful and intentional design.\n\nMilitary dataset\nFortunately, all the data needed to recreate the The Economist stacked bar plot is visible in the plot. Here‚Äôs a dataframe of that data showing the category of military technology for the United States and China.\n\n\n\nshape: (5, 3)\n\n\n\nCategory\nChina\nUnited States\n\n\nstr\ni64\ni64\n\n\n\n\n\"Total battle force\"\n370\n291\n\n\n\"Principle surface combatants\"\n92\n122\n\n\n\"Aircraft-carriers\"\n2\n11\n\n\n\"Combat aircaft\"\n456\n988\n\n\n\"Helicopters\"\n116\n689\n\n\n\n\n\n\n\nAnd now here‚Äôs a Plotly recreation of our stacked bar plot.\n\ndf = (data\n .with_columns(China_pct=pl.col('China') / pl.sum_horizontal('China','United States'),\n               US_pct=pl.col('United States') / pl.sum_horizontal('China','United States'),\n               Category=pl.concat_str([pl.lit('&lt;b&gt;'), pl.col('Category')]))\n )\n\n# Create the percentage stacked bar chart using Plotly Graph Objects\nimport plotly.graph_objects as go\n\n# Adding China's df as percentage with left-aligned text\nfig = go.Figure()\n\n# China's df with actual number label\nfig.add_trace(go.Bar(\n    y=[1, 2, 3, 4, 5],  # Use a numerical y-axis instead of category labels\n    x=df['China_pct'],\n    width=.5,\n    name='China',\n    marker=dict(color='#E3120B'),\n    orientation='h',\n    text=df['China'],  # Actual number label\n    textposition='inside',  # Place text inside the bar\n    insidetextanchor='start'  # Left-align the text inside the bars\n))\n\n# United States' df with actual number label\nfig.add_trace(go.Bar(\n    y=[1, 2, 3, 4, 5],  # Match the same y-values for the US df\n    x=df['US_pct'],\n    name='United States',\n    marker=dict(color='#9a9996'),  # Set the color of the bars\n    width=.5,\n    orientation='h',\n    text=df['United States'],  # Actual number label\n    textposition='inside',  # Place text inside the bar\n    insidetextanchor='end'  # Right-align the text inside the bars\n))\n\n# Adding annotations for the category labels above the bars\nfor i, category in enumerate(df['Category']):\n    fig.add_annotation(\n        x=0,  # Align annotation at the start (left)\n        y=i+1.45,  # Adjust the y value to move the annotation slightly above the bar\n        text=category,  # The category label\n        showarrow=False,  # Disable arrows\n        xanchor='left',  # Left-align the annotation\n        yanchor='middle',  # Center the annotation vertically\n        xshift=0  # No horizontal shift\n    )\n\n# Update layout for percentage stacking and legend positioning\nfig.update_layout(\n    height=600,\n    margin=dict(t=200, b=50),\n    barmode='stack',\n    title=\"&lt;b&gt;China's military tech&lt;/b&gt;&lt;br&gt;&lt;b&gt;still lags the West&lt;/b&gt;\",\n    title_y=.9,\n    title_x=0.03,\n    title_font=dict(size=26),\n    # xaxis_title='Percentage of Total Production',\n    yaxis_title='',  # Remove the category labels from y-axis\n    xaxis=dict(\n        tickvals=[0, 25, 50, 75, 100],  # Set tick marks for percentage\n        showgrid=False,  # Hide grid lines\n        showticklabels=False  # Hide tick labels\n    ),\n    shapes=[\n        dict(\n            type=\"line\",\n            xref=\"paper\",\n            yref=\"paper\",\n            x0=0, y0=1.52,  \n            x1=0.1, y1=1.52,  \n            line=dict(\n                color=RED,  \n                width=10  \n            )\n        )],\n    # Consolidating yaxis settings here\n    yaxis=dict(\n        showticklabels=False,  # Hide the default y-axis labels\n        showgrid=False,\n        tickvals=[1, 2, 3, 4, 5],  # Keep the numerical y-axis values\n        ticktext=['', '', '', '', ''],  # Hide y-axis text to avoid overlap\n        range=[0.5, 5.5]  # Adjust the range to add some padding\n    ),\n    \n    # Adjust the gap between the bars\n    bargap=0.65,  # Increase this to widen the gap between bars\n\n    # Adjust legend positioning\n    legend=dict(\n        x=.96,        # Horizontal position (1 is far right)\n        y=1.04,      # Vertical position (1 is top)\n        xanchor='right',  # Anchors the legend box to the right side of the plot\n        yanchor='top',    # Anchors the legend box to the top of the plot\n        orientation='h'   # Set the legend items to be horizontal\n    ),\n    \n    plot_bgcolor='#E9EDF0',\n    paper_bgcolor='#E9EDF0',\n\n    # Set the font to \"Inter\"\n    font=dict(\n        family=\"Inter\",\n        size=12,  # Adjust the size as needed\n        color=\"Black\"  # Adjust the color as needed\n    ),\n)\n\nfig.add_annotation(\n    dict(\n           text=\"Sources: IISS; US Department of Defence\",\n           x=-0.005,  # x position (0 means far left)\n           y=-0.12,  # y position (adjust as necessary)\n           xref=\"paper\",\n           yref=\"paper\",\n           showarrow=False,  # No arrow\n           font=dict(\n               family=\"Inter\",  # Font family\n               # size=12,  # Font size\n               color=\"#9a9996\"  # Font color\n           ),\n           align=\"left\"\n       ),\n)\n\nfig.add_annotation(\n    dict(\n           text=\"&lt;b&gt; Navy balance, December 2022 or latest available\",\n           x=-0.01,  # x position (0 means far left)\n           y=1.2,  # y position (adjust as necessary)\n           xref=\"paper\",\n           yref=\"paper\",\n           showarrow=False,  # No arrow\n           font=dict(\n               family=\"Inter\",  # Font family\n               size=18,  # Font size\n               color=\"#1a1a1a\"  # Font color\n           ),\n           align=\"left\"\n       ),\n)\n\nfig.add_layout_image(\n    dict(\n        source=f\"data:image/png;base64,{encoded_image}\",\n        xref=\"paper\",  # Reference the x position relative to the plotting area\n        yref=\"paper\",  # Reference the y position relative to the plotting area\n        x=.95,  # x-coordinate (1 means far right)\n        y=-0.12,  # y-coordinate (0 means bottom)\n        xanchor=\"right\",  # Anchor the image from the right\n        yanchor=\"bottom\",  # Anchor the image from the bottom\n        sizex=0.2,  # Set the width of the image (adjust as necessary)\n        sizey=0.2,  # Set the height of the image (adjust as necessary)\n        layer=\"above\"  # Make sure the image is displayed above the plot\n    )\n)\n\nfig.show()\n\n                                                \n\n\n\nAnd there you have it! We‚Äôve successfully recreated charts inspired by The Economist magazine using Plotly for graphing and Polars for data manipulation.\nIf you need custom plots for your data, whether for print or digital media, feel free to reach out to us!"
  },
  {
    "objectID": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html",
    "href": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html",
    "title": "Tranforming timeseries data with group by and group by dynamic in polars",
    "section": "",
    "text": "a brand of yogurt\nPolars has become my go-to library for data analysis. Each client project brings new insights into the powerful functionality Polars offers. Recently, I worked on a project for a supermarket that required processing data related to yogurt stock quantities."
  },
  {
    "objectID": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-problem",
    "href": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-problem",
    "title": "Tranforming timeseries data with group by and group by dynamic in polars",
    "section": "The problem",
    "text": "The problem\nThe supermarket‚Äôs data included multiple quantity entries for the same yogurt brand within a single month. The goal was to aggregate these quantities into a single value per month and standardize the date to the first day of that month.\nFor example, the data for a yogurt brand like Chobani in February might look like this:\n\nFeb-02-2025 = 30 units\n\nFeb-08-2025 = 20 units\n\nFeb-15-2025 = 50 units\n\nThe desired output for February would aggregate these values into:\n\nFeb-01-2025 = 100 units\n\nThis aggregation needed to be repeated for every yogurt brand sold by the supermarket."
  },
  {
    "objectID": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-dataset",
    "href": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-dataset",
    "title": "Tranforming timeseries data with group by and group by dynamic in polars",
    "section": "The dataset",
    "text": "The dataset\nTo demonstrate how I solved this problem, I‚Äôll use a representative dataset (not the actual client data).\n\n\n\nshape: (49, 3)\n\n\n\nDate\nYogurt\nQuantity\n\n\ndate\nstr\ni64\n\n\n\n\n2025-02-05\n\"Yoplait\"\n54\n\n\n2025-02-11\n\"Yoplait\"\n54\n\n\n2025-02-14\n\"Yoplait\"\n54\n\n\n2025-02-19\n\"Yoplait\"\n54\n\n\n2025-02-26\n\"Yoplait\"\n54\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-06-03\n\"Wallaby Organic\"\n48\n\n\n2025-06-03\n\"Yoplait\"\n54\n\n\n2025-06-03\n\"Chobani\"\n120\n\n\n2025-06-23\n\"Dannon\"\n12\n\n\n2025-06-24\n\"Chobani\"\n120"
  },
  {
    "objectID": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-solution",
    "href": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-solution",
    "title": "Tranforming timeseries data with group by and group by dynamic in polars",
    "section": "The solution",
    "text": "The solution\nI‚Äôll show two approaches to solving this problem. The first method uses group_by, while the second uses group_by_dynamic. Afterward, I‚Äôll verify that both methods produce identical results. More importantly, I‚Äôll compare their performance by using the %%timeit cell magic command to identify the faster solution.\n\nSolution with group by dynamic\nWhen resampling time series data, group_by_dynamic simplifies selecting a specific time period (e.g., weekly, monthly, quarterly) and resampling the data to perform aggregations based on the chosen interval. However, group_by_dynamic does not support grouping by multiple columns. While this limitation may make it unsuitable for addressing the client‚Äôs problem directly, a workaround is available.\nLet‚Äôs process the data for a single yogurt brand, Yoplait to see if the solution is working the way we expect it. Then we‚Äôll repeat the process for all yogurt brands. Here‚Äôs is the unprocessed data for Yoplait yogurt.\n\ndata.filter(pl.col('Yogurt') == \"Yoplait\")\n\n\nshape: (18, 3)\n\n\n\nDate\nYogurt\nQuantity\n\n\ndate\nstr\ni64\n\n\n\n\n2025-02-05\n\"Yoplait\"\n54\n\n\n2025-02-11\n\"Yoplait\"\n54\n\n\n2025-02-14\n\"Yoplait\"\n54\n\n\n2025-02-19\n\"Yoplait\"\n54\n\n\n2025-02-26\n\"Yoplait\"\n54\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-04-30\n\"Yoplait\"\n54\n\n\n2025-05-09\n\"Yoplait\"\n54\n\n\n2025-05-16\n\"Yoplait\"\n54\n\n\n2025-05-26\n\"Yoplait\"\n54\n\n\n2025-06-03\n\"Yoplait\"\n54\n\n\n\n\n\n\n\nHere is the processed data for Yoplait yogurt. The aggregations have been completed, resulting in a reduced number of rows in the dataframe.\n\n(data\n .filter(pl.col('Yogurt') == \"Yoplait\")\n .group_by_dynamic('Date', every='1mo')\n .agg(pl.sum('Quantity'), pl.first('Yogurt'))\n)\n\n\nshape: (5, 3)\n\n\n\nDate\nQuantity\nYogurt\n\n\ndate\ni64\nstr\n\n\n\n\n2025-02-01\n270\n\"Yoplait\"\n\n\n2025-03-01\n270\n\"Yoplait\"\n\n\n2025-04-01\n216\n\"Yoplait\"\n\n\n2025-05-01\n162\n\"Yoplait\"\n\n\n2025-06-01\n54\n\"Yoplait\"\n\n\n\n\n\n\n Notice that I have selected a 1-month time period for the every parameter. However, there is an unresolved issue in our solution: we have multiple dates instead of a single date representing the first day of each month. To address this, I will introduce another parameter, start_by, and set its value to \"window\". This ensures that all dates are converted to the first day of their respective months.\n\n(data\n .filter(pl.col('Yogurt') == \"Yoplait\")\n .group_by_dynamic('Date', every='1mo', start_by='window')\n .agg(pl.sum('Quantity'), pl.first('Yogurt'))\n)\n\n\nshape: (5, 3)\n\n\n\nDate\nQuantity\nYogurt\n\n\ndate\ni64\nstr\n\n\n\n\n2025-02-01\n270\n\"Yoplait\"\n\n\n2025-03-01\n270\n\"Yoplait\"\n\n\n2025-04-01\n216\n\"Yoplait\"\n\n\n2025-05-01\n162\n\"Yoplait\"\n\n\n2025-06-01\n54\n\"Yoplait\"\n\n\n\n\n\n\n Having achieved the desired results for Yoplait yogurt, I can now process the data for the other brands. Instead of processing each brand individually, I will use a for loop to automate the task.\nFirst, I‚Äôll create a list of all the yogurt brands contained in the dataset.\n\nyogurt_list = data['Yogurt'].unique().to_list()\nyogurt_list\n\n['Brown Cow',\n \"Siggi's\",\n 'Oikos',\n 'Chobani',\n 'Yoplait',\n 'Wallaby Organic',\n 'Activia',\n 'Stonyfield Organic',\n 'Dannon',\n 'Fage',\n 'Noosa']\n\n\nAnd now here‚Äôs the code that implements the for loop.\n\ndfs = []\nfor item in yogurt_list:\n    df = (data\n    .filter(pl.col('Yogurt') == item)\n    .group_by_dynamic('Date', every='1mo', start_by='window')\n    .agg(pl.sum('Quantity'), pl.first('Yogurt'))\n    )\n    dfs.append(df)\ndf_1 = pl.concat(dfs)\ndf_1\n\n\nshape: (23, 3)\n\n\n\nDate\nQuantity\nYogurt\n\n\ndate\ni64\nstr\n\n\n\n\n2025-04-01\n6\n\"Brown Cow\"\n\n\n2025-04-01\n6\n\"Siggi's\"\n\n\n2025-04-01\n144\n\"Oikos\"\n\n\n2025-05-01\n144\n\"Oikos\"\n\n\n2025-03-01\n444\n\"Chobani\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-06-01\n12\n\"Dannon\"\n\n\n2025-05-01\n24\n\"Fage\"\n\n\n2025-03-01\n12\n\"Noosa\"\n\n\n2025-04-01\n12\n\"Noosa\"\n\n\n2025-05-01\n12\n\"Noosa\"\n\n\n\n\n\n\n\nTo verify that the above code worked correctly, let display the results for Yoplait yogurt.\n\ndf_1.filter(pl.col('Yogurt') == \"Yoplait\")\n\n\nshape: (5, 3)\n\n\n\nDate\nQuantity\nYogurt\n\n\ndate\ni64\nstr\n\n\n\n\n2025-02-01\n270\n\"Yoplait\"\n\n\n2025-03-01\n270\n\"Yoplait\"\n\n\n2025-04-01\n216\n\"Yoplait\"\n\n\n2025-05-01\n162\n\"Yoplait\"\n\n\n2025-06-01\n54\n\"Yoplait\"\n\n\n\n\n\n\n\nGreat! The results are what we expected.\n\n\nSolution with group by\nFortunately, with group_by, it is possible to aggregate data across multiple columns. This allows me to process the data for all yogurt brands without using a for loop. However, I first need to create a new column, Month, to use as one of the grouping columns in group_by. As before, I will start by processing the data for a single yogurt brand, Yoplait.\n\n(data\n .filter(pl.col('Yogurt') == \"Yoplait\")\n .with_columns(Month=pl.col('Date').dt.month())\n .group_by('Yogurt','Month')\n .agg(pl.sum('Quantity'),\n       pl.first('Date'))\n .drop('Month')\n .with_columns(pl.col(\"Date\").dt.truncate(\"1mo\"))\n )\n\n\nshape: (5, 3)\n\n\n\nYogurt\nQuantity\nDate\n\n\nstr\ni64\ndate\n\n\n\n\n\"Yoplait\"\n270\n2025-02-01\n\n\n\"Yoplait\"\n270\n2025-03-01\n\n\n\"Yoplait\"\n162\n2025-05-01\n\n\n\"Yoplait\"\n54\n2025-06-01\n\n\n\"Yoplait\"\n216\n2025-04-01\n\n\n\n\n\n\n\nNotice that I have used two columns, Yogurt and Month, in group_by to aggregate quantities based on this two-column combination. Since Month has served its purpose, I can drop it as it is no longer needed. However, the date values are not in the expected format. To resolve this issue, I will use truncate and set the value to \"1mo\" to adjust the values in the Date column by one month.\n\n(data\n .filter(pl.col('Yogurt') == \"Yoplait\")\n .with_columns(Month=pl.col('Date').dt.month())\n .group_by('Yogurt','Month')\n .agg(pl.sum('Quantity'),\n       pl.first('Date'))\n .drop('Month')\n .with_columns(pl.col(\"Date\").dt.truncate(\"1mo\"))\n )\n\n\nshape: (5, 3)\n\n\n\nYogurt\nQuantity\nDate\n\n\nstr\ni64\ndate\n\n\n\n\n\"Yoplait\"\n54\n2025-06-01\n\n\n\"Yoplait\"\n162\n2025-05-01\n\n\n\"Yoplait\"\n216\n2025-04-01\n\n\n\"Yoplait\"\n270\n2025-02-01\n\n\n\"Yoplait\"\n270\n2025-03-01\n\n\n\n\n\n\nNow that we have the expected results, all that‚Äôs left to process the data for all yogurt brands is to remove the line of code containing filter.\n\ndf_2 = (data\n .with_columns(Month=pl.col('Date').dt.month())\n .group_by('Yogurt','Month')\n .agg(pl.sum('Quantity'),\n       pl.first('Date'))\n .drop('Month')\n .with_columns(pl.col(\"Date\").dt.truncate(\"1mo\"))\n )\ndf_2\n\n\nshape: (23, 3)\n\n\n\nYogurt\nQuantity\nDate\n\n\nstr\ni64\ndate\n\n\n\n\n\"Siggi's\"\n6\n2025-04-01\n\n\n\"Noosa\"\n12\n2025-04-01\n\n\n\"Yoplait\"\n162\n2025-05-01\n\n\n\"Fage\"\n24\n2025-05-01\n\n\n\"Oikos\"\n144\n2025-04-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Chobani\"\n240\n2025-04-01\n\n\n\"Activia\"\n504\n2025-05-01\n\n\n\"Noosa\"\n12\n2025-05-01\n\n\n\"Dannon\"\n12\n2025-06-01\n\n\n\"Yoplait\"\n270\n2025-02-01"
  },
  {
    "objectID": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#comparing-the-two-methods",
    "href": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#comparing-the-two-methods",
    "title": "Tranforming timeseries data with group by and group by dynamic in polars",
    "section": "Comparing the two methods",
    "text": "Comparing the two methods\nMultiple factors can be used to determine which code is better, such as ease of writing. However, I will focus on determining which code processes the data faster. Let‚Äôs test which approach performs better.\nWith group by dynamic\n\n%%timeit\n\ndfs = []\nfor item in yogurt_list:\n    df = (data\n    .filter(pl.col('Yogurt') == item)\n    .group_by_dynamic('Date', every='1mo', start_by='window')\n    .agg(pl.sum('Quantity'), pl.first('Yogurt'))\n    )\n    dfs.append(df)\ndf_1 = pl.concat(dfs)\n\n935 Œºs ¬± 3.08 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nWith group by\n\n%%timeit\n\n(data\n .with_columns(Month=pl.col('Date').dt.month())\n .group_by('Yogurt','Month')\n .agg(pl.sum('Quantity'),\n       pl.first('Date'))\n .drop('Month')\n .with_columns(pl.col(\"Date\").dt.truncate(\"1mo\"))\n )\n\n252 Œºs ¬± 1.1 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nYou can see that group_by processes the data faster, making it the better method. This isn‚Äôt surprising, actually, because it doesn‚Äôt involve using a for loop. It‚Äôs always better to avoid loops when working with dataframes, as this allows your code to be executed in Rust, the language in which Polars was written. When you use for loops, your code is executed in Python, which is slower than Rust.\nCheck out the new Polars for Finance course we published to learn how to process and analyze stock data."
  },
  {
    "objectID": "blog/what-tool-should-you-use-as-a-data-analyst/index.html",
    "href": "blog/what-tool-should-you-use-as-a-data-analyst/index.html",
    "title": "What tool should you use as a data analyst?",
    "section": "",
    "text": "all the tools you‚Äôll want to use\n\n\n\nData analysis is a hot field nowadays. Companies are opening up new data analyst positions, and many people want to become data analysts.\nIt‚Äôs standard knowledge that data analysts work with data. What‚Äôs not standard knowledge is the tools they use. There are just so many data analysis tools out there, and it‚Äôs hard to know what to pick.\nSo what tool should you use in your data analysis job? Tableau? PowerBI?, Jupyer Notebook? Marimo Notebook? And oh, what about git, should you worry about it? What dataframe library should you use, Polars, Pandas, or Ibis? Then there‚Äôs the language wars; R vs Python vs Julia. It‚Äôs just so damn confusing.\nThe answer to what tool you should use may sound unsatisfying, but trust me it‚Äôs the right answer. Are you ready for it?\n\nThe right tool to use for your data analysis work is the tool you know how to use. That‚Äôs it.\n\nIf you can automate tasks with R better than you can automate them with Python, then use R (or vice versa). If wrangling data with Pandas is easier for you than doing it with Polars, use Pandas. If it takes you less time to create visualizations with Tableau than it does PowerBI, use Tableau.\nThe point is that work should not be too difficult to do. When you pull out of your toolbox, always start with the tools you know how to use since you‚Äôll start doing the work immediately rather than figuring out how to use the tool.\nHowever, I‚Äôd encourage you to experiment with new tools every once in a while. You might stumble on a new tool that does the work faster and easier than the old tool you‚Äôre familiar with. It happened to me when I switched from Polars to Pandas.\n\nI came to Polars for the speed and stayed for the syntax.\n\nIt was the best choice, and I‚Äôve never looked back. Had I stuck with Pandas (the dataframe library I knew how to use), I wouldn‚Äôt have experienced how great Polars is."
  }
]