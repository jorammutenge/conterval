[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Conterval",
    "section": "",
    "text": "a data consulting company\n\n\n\n\n\nOur Services\n\n\nData Analysis\n\n\nData Cleaning\n\n\nData Engineering\n\n\nData Visualization\n\n\nAnalytics Engineering\n\n\nExtract Transform Load (ETL)\n\n\n\n\n\nContact Us\n\n\nüìß contact@conterval.com"
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "",
    "text": "In the last client project I worked on, I learned something about the group_by_dynamic function in Polars. While what I learned was surprising, the fact that I learned it during the project was not. This aligns with the philosophy of ‚Äúlet the work be the practice‚Äù that Cal Newport advocates, and I‚Äôm proud to say I follow it. Most people spend time learning about a particular technology before they use it in a project. Cal Newport‚Äôs philosophy suggests combining learning with doing. By practicing through actual work, you gain mastery. By the end of this project, my proficiency in Polars increased dramatically."
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#a-brief-overview-of-the-project",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#a-brief-overview-of-the-project",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "A brief overview of the project",
    "text": "A brief overview of the project\nThe client had forecast data for 2025 for electronic gadgets in monthly buckets, but she wanted it converted into weekly buckets. The dates in the data she presented all started at the beginning of each month. For instance, the first row contained the date January 1, 2025. Below is the forecast data from the client.\n\n\n\nshape: (96, 3)\n\n\n\nDate\nGadget\nForecast\n\n\ndate\nstr\ni16\n\n\n\n\n2025-01-01\n\"Headphones\"\n3439\n\n\n2025-01-01\n\"Keyboard\"\n1652\n\n\n2025-01-01\n\"Monitor\"\n311\n\n\n2025-01-01\n\"Mouse\"\n1139\n\n\n2025-01-01\n\"Printer\"\n123\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-12-01\n\"Mouse\"\n1385\n\n\n2025-12-01\n\"Printer\"\n166\n\n\n2025-12-01\n\"Smartwatch\"\n678\n\n\n2025-12-01\n\"Tablet\"\n496\n\n\n2025-12-01\n\"Webcam\"\n512"
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#how-we-solved-the-problem",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#how-we-solved-the-problem",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "How we solved the problem",
    "text": "How we solved the problem\nAt first, I thought I could solve the problem by simply dividing each forecast value by 7, the number of days in a week. However, a colleague quickly reminded me that not all months are created equal‚Äîsome have more days than others. So, I quickly abandoned that approach and searched for an alternative.\nThe solution that worked involved creating, from scratch, a single-column dataframe containing all the days of the year 2025, from January to December. We joined this dataframe with the client‚Äôs forecast data and then applied group_by_dynamic. It worked like a charm, but it also exposed something I hadn‚Äôt been fully aware of regarding group_by_dynamic."
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#creating-the-date-dataframe.",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#creating-the-date-dataframe.",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "Creating the date dataframe.",
    "text": "Creating the date dataframe.\nUsing datetime_range, I created a dataframe containing timeseries values for the entire year of 2025. The interval was set to 1 day, ensuring that every single date in the year is included. Below is the resulting timeseries dataframe. Then I extracted the month values from the dates to create a new column Month.\n\nfrom datetime import datetime\ndate_df = pl.DataFrame(\n    {\n        \"Date\": pl.datetime_range(\n            start=datetime(2025, 1, 1),\n            end=datetime(2025, 12, 31),\n            interval=\"1d\",\n            eager=True,\n        )\n    }\n).with_columns(pl.col('Date').dt.date(),\n               Month=pl.col('Date').dt.month())\ndate_df\n\n\nshape: (365, 2)\n\n\n\nDate\nMonth\n\n\ndate\ni8\n\n\n\n\n2025-01-01\n1\n\n\n2025-01-02\n1\n\n\n2025-01-03\n1\n\n\n2025-01-04\n1\n\n\n2025-01-05\n1\n\n\n‚Ä¶\n‚Ä¶\n\n\n2025-12-27\n12\n\n\n2025-12-28\n12\n\n\n2025-12-29\n12\n\n\n2025-12-30\n12\n\n\n2025-12-31\n12"
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#joining-the-two-dataframes",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#joining-the-two-dataframes",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "Joining the two dataframes",
    "text": "Joining the two dataframes\nBefore joining the two DataFrames, I converted the date values in the forecast dataset to month-only values. This ensured a unique common value between both dataframes, enabling the join to proceed. The dataframe below illustrates the transformation from date values to month-only values.\n\ndf = (data\n .with_columns(pl.col('Date').dt.month())\n )\ndf\n\n\nshape: (96, 3)\n\n\n\nDate\nGadget\nForecast\n\n\ni8\nstr\ni16\n\n\n\n\n1\n\"Headphones\"\n3439\n\n\n1\n\"Keyboard\"\n1652\n\n\n1\n\"Monitor\"\n311\n\n\n1\n\"Mouse\"\n1139\n\n\n1\n\"Printer\"\n123\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n12\n\"Mouse\"\n1385\n\n\n12\n\"Printer\"\n166\n\n\n12\n\"Smartwatch\"\n678\n\n\n12\n\"Tablet\"\n496\n\n\n12\n\"Webcam\"\n512\n\n\n\n\n\n\n\nThe two dataframes were joined using a left join. Also, I decided to divide the forecast values by 4 since most months have at least 4 weeks Here‚Äôs an example of the resulting dataframe for Headphones showing the monthly and weekly forecast.\n\n(date_df\n .join(df, left_on='Month', right_on='Date', how='left')\n .drop('Month')\n .with_columns(Weekly_Forecast=pl.col('Forecast').truediv(4).round(0).cast(pl.Int16))\n .sort('Date')\n .filter(pl.col('Gadget') == \"Headphones\")\n )\n\n\nshape: (365, 4)\n\n\n\nDate\nGadget\nForecast\nWeekly_Forecast\n\n\ndate\nstr\ni16\ni16\n\n\n\n\n2025-01-01\n\"Headphones\"\n3439\n860\n\n\n2025-01-02\n\"Headphones\"\n3439\n860\n\n\n2025-01-03\n\"Headphones\"\n3439\n860\n\n\n2025-01-04\n\"Headphones\"\n3439\n860\n\n\n2025-01-05\n\"Headphones\"\n3439\n860\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-12-27\n\"Headphones\"\n2992\n748\n\n\n2025-12-28\n\"Headphones\"\n2992\n748\n\n\n2025-12-29\n\"Headphones\"\n2992\n748\n\n\n2025-12-30\n\"Headphones\"\n2992\n748\n\n\n2025-12-31\n\"Headphones\"\n2992\n748"
  },
  {
    "objectID": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#converting-to-weekly-buckets",
    "href": "blog/what-I-learned-about-group-by-dynamic-in-polars/index.html#converting-to-weekly-buckets",
    "title": "What I learned about group by dynamic in polars while working on a client‚Äôs project",
    "section": "Converting to weekly buckets",
    "text": "Converting to weekly buckets\nThe dates are still in days, but the client wants them in weeks so I used groub_by_dynamic with an interval of 7 days to convert them into weekly buckets. Below is the resulting dataframe.\n\nfrom great_tables import loc, style\n\nprocessed_df = (date_df\n .join(df, left_on='Month', right_on='Date', how='left')\n .drop('Month')\n .with_columns(Weekly_Forecast=pl.col('Forecast').truediv(4).round(0).cast(pl.Int16))\n .sort('Date')\n .filter(pl.col('Gadget') == \"Headphones\")\n .group_by_dynamic('Date', every='7d')\n .agg(pl.first('Gadget', 'Forecast', 'Weekly_Forecast'))\n )\n\nprocessed_df[0:10].style.tab_style(\n    style.fill(\"yellow\"),\n    loc.body(\n        rows=pl.col(\"Date\").dt.year() == 2024,\n    ),\n)\n\n\n\n\n\n\n\nDate\nGadget\nForecast\nWeekly_Forecast\n\n\n\n\n2024-12-26\nHeadphones\n3439\n860\n\n\n2025-01-02\nHeadphones\n3439\n860\n\n\n2025-01-09\nHeadphones\n3439\n860\n\n\n2025-01-16\nHeadphones\n3439\n860\n\n\n2025-01-23\nHeadphones\n3439\n860\n\n\n2025-01-30\nHeadphones\n3439\n860\n\n\n2025-02-06\nHeadphones\n2620\n655\n\n\n2025-02-13\nHeadphones\n2620\n655\n\n\n2025-02-20\nHeadphones\n2620\n655\n\n\n2025-02-27\nHeadphones\n2620\n655\n\n\n\n\n\n\n\n\n\nDo you notice the peculiarity that group_by_dynamic introduces? None of our original DataFrames contained the year 2024, yet after using group_by_dynamic, we now see 2024. What‚Äôs going on here? I was initially unaware of this behavior. It turns out that group_by_dynamic shifts date values by the interval specified in the every parameter. Since we used a 7-day interval, the date values were moved 7 days back, causing the appearance of 2024.\nBut wait‚Äîthe client specifically needs forecast data for 2025. How can we address this? Thankfully, the developers of Polars anticipated this issue and provided a solution. As outlined in the documentation, adding the start_by parameter with the value \"datapoint\" to group_by_dynamic resolves the problem. With this adjustment, the year 2024 disappeared entirely.\n\n(date_df\n .join(df, left_on='Month', right_on='Date', how='left')\n .drop('Month')\n .filter(pl.col('Gadget') == 'Monitor')\n .with_columns(Weekly_Forecast=pl.col('Forecast').truediv(4).round(0).cast(pl.Int16))\n .sort('Date')\n .group_by_dynamic('Date', every='7d', start_by='datapoint')\n .agg(pl.first('Gadget', 'Forecast', 'Weekly_Forecast'))\n )\n\n\nshape: (53, 4)\n\n\n\nDate\nGadget\nForecast\nWeekly_Forecast\n\n\ndate\nstr\ni16\ni16\n\n\n\n\n2025-01-01\n\"Monitor\"\n311\n78\n\n\n2025-01-08\n\"Monitor\"\n311\n78\n\n\n2025-01-15\n\"Monitor\"\n311\n78\n\n\n2025-01-22\n\"Monitor\"\n311\n78\n\n\n2025-01-29\n\"Monitor\"\n311\n78\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-12-03\n\"Monitor\"\n325\n81\n\n\n2025-12-10\n\"Monitor\"\n325\n81\n\n\n2025-12-17\n\"Monitor\"\n325\n81\n\n\n2025-12-24\n\"Monitor\"\n325\n81\n\n\n2025-12-31\n\"Monitor\"\n325\n81\n\n\n\n\n\n\n\nWith the date issue resolved, we can now proceed to develop the code needed to create the final dataset for presentation to the client.\n\ngadget_list = df['Gadget'].unique().to_list()\n\nbucket_dfs = []\nfor gadget in gadget_list:\n    bucket_df = (date_df\n        .join(df, left_on='Month', right_on='Date', how='left')\n        .drop('Month')\n        .filter(pl.col('Gadget') == gadget)\n        .with_columns(Weekly_Forecast=pl.col('Forecast').truediv(4).round(0).cast(pl.Int16))\n        .sort('Date')\n        .group_by_dynamic('Date', every='7d', start_by='datapoint')\n        .agg(pl.first('Gadget', 'Forecast', 'Weekly_Forecast'))\n        )\n    bucket_dfs.append(bucket_df)\n\nall_bucket_df = pl.concat(bucket_dfs).drop('Forecast')\nall_bucket_df\n\n\nshape: (424, 3)\n\n\n\nDate\nGadget\nWeekly_Forecast\n\n\ndate\nstr\ni16\n\n\n\n\n2025-01-01\n\"Keyboard\"\n413\n\n\n2025-01-08\n\"Keyboard\"\n413\n\n\n2025-01-15\n\"Keyboard\"\n413\n\n\n2025-01-22\n\"Keyboard\"\n413\n\n\n2025-01-29\n\"Keyboard\"\n413\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-12-03\n\"Mouse\"\n346\n\n\n2025-12-10\n\"Mouse\"\n346\n\n\n2025-12-17\n\"Mouse\"\n346\n\n\n2025-12-24\n\"Mouse\"\n346\n\n\n2025-12-31\n\"Mouse\"\n346\n\n\n\n\n\n\n\nThe lesson to remember is that group_by_dynamic will move the date values back by the specified interval you set in the every parameter. If you want to maintain the date values in your dataset, you must add ananother parameter start_at and set it to \"datapoint\".\nReach out if you need help with your data problems. Also, take a look at our Polars course to improve your data analysis skills using this fast Python library."
  },
  {
    "objectID": "blog/set-theory-for-data-analysis/index.html",
    "href": "blog/set-theory-for-data-analysis/index.html",
    "title": "Using set theory to speed up your data analysis",
    "section": "",
    "text": "Most data analysis tasks involve joining tables to get more data or filter out specific data. But what happens when the data you‚Äôre working with isn‚Äôt in a format that allows easy table joins? In such cases, you can turn to sets. That‚Äôs right‚Äîthe same sets you learned about in grade school can be incredibly useful for analyzing data. By applying your knowledge on set theory, you can effectively handle and analyze complex datasets."
  },
  {
    "objectID": "blog/set-theory-for-data-analysis/index.html#a-refresher-on-sets",
    "href": "blog/set-theory-for-data-analysis/index.html#a-refresher-on-sets",
    "title": "Using set theory to speed up your data analysis",
    "section": "A refresher on sets",
    "text": "A refresher on sets\nIn case you‚Äôve forgotten the fundamentals of set operations, allow me to bring you up to speed. First, let‚Äôs define what a set is:\n\nA set is a collection of zero or more items. Or, if you want to be more technical, a set is a collection of zero or more object references that point to hashable objects.\n\nSets are mutable, meaning you can add or remove items from them.\n\nSets are unordered, meaning they have no index positions and cannot be sliced like lists."
  },
  {
    "objectID": "blog/set-theory-for-data-analysis/index.html#set-operators",
    "href": "blog/set-theory-for-data-analysis/index.html#set-operators",
    "title": "Using set theory to speed up your data analysis",
    "section": "Set operators",
    "text": "Set operators\nIntersection\nThe intersection operation is one of the most commonly used in sets. It identifies and retrieves the elements that are shared across all the sets being compared. In Python, the symbol & is used for intersection. Suppose you had the following two sets.\n\n\n\n\nsets of data analysts and engineers\n\n\n\n\ndata_engineers = set(['Jeremie', 'Joram', 'Ollie', 'Ashwin'])\ndata_analysts = set(['Joram', 'Ollie', 'Pallavi', 'Maura'])\n\nHere‚Äôs how you would find out the people who are both data analysts and data engineers.\n\ndata_engineers & data_analysts\n\n{'Joram', 'Ollie'}\n\n\nNow we see that Joram and Ollie are the superstars who can do both data analysis and data engineering.\nUnion\nThe union operation gathers all the elements from both sets, ensuring that each element appears only once in the resulting set. The | symbol represents this operation. Using the union operator will give me all the names of data analysts and engineers.\n\ndata_engineers | data_analysts\n\n{'Ashwin', 'Jeremie', 'Joram', 'Maura', 'Ollie', 'Pallavi'}\n\n\nDifference\nThe difference operation identifies elements that exist in one set but not in the other. It is similar to subtraction in numbers. The symbol - represents this operation. The code below will give me names of people who are only data engineers.\n\ndata_engineers - data_analysts\n\n{'Ashwin', 'Jeremie'}\n\n\n\n\n\n\n\n\nNote\n\n\n\nSet difference produces different results when the order is reversed. So, A - B is not the same as B - A.\n\n\n\ndata_analysts - data_engineers\n\n{'Maura', 'Pallavi'}\n\n\nSymmetric difference\nThis operation retrieves every element in set A and every element in set B, but excludes elements that are present in both sets. In short, it‚Äôs the subtraction of the intersection from the union set. The symbol ^ is used for set difference.\nHere‚Äôs how I would get all the names of people who are only data analysts or data engineers. The superstars ‚Äì those who can do both won‚Äôt be included.\n\ndata_analysts ^ data_engineers\n\n{'Ashwin', 'Jeremie', 'Maura', 'Pallavi'}\n\n\nAddition\nThis adds or inserts an element into the set. Let me display the set that contains data analysts.\n\ndata_analysts\n\n{'Joram', 'Maura', 'Ollie', 'Pallavi'}\n\n\nNow let me add the name ‚ÄúDavid‚Äù to that set.\n\ndata_analysts.add('David')\ndata_analysts\n\n{'David', 'Joram', 'Maura', 'Ollie', 'Pallavi'}"
  },
  {
    "objectID": "blog/set-theory-for-data-analysis/index.html#analyze-sales-data",
    "href": "blog/set-theory-for-data-analysis/index.html#analyze-sales-data",
    "title": "Using set theory to speed up your data analysis",
    "section": "Analyze sales data",
    "text": "Analyze sales data\nNow that your memory is refreshed on sets, let‚Äôs apply this knowledge to analyze real sales data. The sales data is stored in a CSV file, so I‚Äôll use Python‚Äôs csv library to read it. To start, here‚Äôs how the data would look in Excel.\n\n\n\nshape: (1_000, 8)\n\n\n\nAccount Number\nAccount Name\nsku\ncategory\nquantity\nunit price\next price\ndate\n\n\ni64\nstr\nstr\nstr\ni64\nf64\nf64\ndatetime[Œºs]\n\n\n\n\n803666\n\"Fritsch-Glover\"\n\"HX-24728\"\n\"Hat\"\n1\n98.98\n98.98\n2014-09-28 11:56:02\n\n\n64898\n\"O'Conner Inc\"\n\"LK-02338\"\n\"Sweater\"\n9\n34.8\n313.2\n2014-04-24 16:51:22\n\n\n423621\n\"Beatty and Sons\"\n\"ZC-07383\"\n\"Sweater\"\n12\n60.24\n722.88\n2014-09-17 17:26:22\n\n\n137865\n\"Gleason, Bogisich and Franecki\"\n\"QS-76400\"\n\"Sweater\"\n5\n15.25\n76.25\n2014-01-30 07:34:02\n\n\n435433\n\"Morissette-Heathcote\"\n\"RU-25060\"\n\"Sweater\"\n19\n51.83\n984.77\n2014-08-24 06:18:12\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n29068\n\"Brekke and Sons\"\n\"FT-50146\"\n\"Sweater\"\n2\n46.48\n92.96\n2014-08-10 16:20:32\n\n\n77116\n\"Lang-Wunsch\"\n\"IC-59308\"\n\"Socks\"\n19\n29.25\n555.75\n2013-11-20 13:32:45\n\n\n23749\n\"Bogisich and Sons\"\n\"IC-59308\"\n\"Socks\"\n18\n54.79\n986.22\n2014-03-10 08:11:59\n\n\n172519\n\"Kutch, Cormier and Harber\"\n\"RU-25060\"\n\"Sweater\"\n15\n62.53\n937.95\n2014-04-11 02:50:03\n\n\n914594\n\"Roberts, Volkman and Batz\"\n\"LK-02338\"\n\"Sweater\"\n11\n86.4\n950.4\n2014-02-14 20:10:42\n\n\n\n\n\n\n\nThe task\nThe table above contains 1,000 sales transactions. Your task is to generate a sales report for your boss that answers the following questions:\n\nHow many customers have bought socks?\nHow many customers have bought hats?\nWho has bought socks but not hats?\nWhich customers have bought socks, hats and sweaters?\n\nOf course you can do this task in Excel by sorting and filtering your way to the report, but that‚Äôs time-consuming. I want to show you an efficient way to create the report that you can run month, weekly, even daily without having to redo the work.\nCreating the report\nFirst let‚Äôs load the CSV file using the csv library.\n\nimport csv\n\nsales_file = open(f\"{Path('../../')}/datasets/sample_sales.csv\", \"rt\")\nreader = csv.reader(sales_file)\n\nNow let‚Äôs create the empty sets for the three product categories contained the CSV file.\n\nsocks = set()\nhats = set()\nsweaters = set()\n\nFinally, let‚Äôs creat the logic that will populate the sets we created with values.\n\nfor row in reader:\n    customer = (row[0],row[1])\n    category = row[3]\n    if category == 'Socks':\n        socks.add(customer)\n    if category == 'Hat':\n        hats.add(customer)\n    if category == 'Sweater':\n        sweaters.add(customer)\n\nsales_file.close()\n\nYou‚Äôve loaded the data and applied the logic to populate the sets representing the product categories. You can now answer the questions that your boss wants answered in the report.\nHere‚Äôs how you would find the number of customes who bought socks.\n\nlen(socks)\n\n271\n\n\nWhat about the number of customers who bough hats?\n\nlen(hats)\n\n170\n\n\nYou‚Äôre now confident that your code works, so it‚Äôs time to create the report. Below is a Venn diagram that highlights the key questions to address in the report.\n\n\n\n\nvenn diagram of product category sales\n\n\n\nHere‚Äôs the full Python code you can run to create the report for you boss. It includes the names of customers who bought all three products. These are the valued customers.\n\nimport csv\n\n# Using with open to automatically handle file closing\nwith open(f\"{Path('../../')}/datasets/sample_sales.csv\", \"rt\") as sales_file:\n    reader = csv.reader(sales_file)\n\n    # Initialize the sets\n    socks = set()\n    hats = set()\n    sweaters = set()\n\n    # Write the logic\n    for row in reader:\n        customer = (row[0], row[1])\n        category = row[3]\n        if category == 'Socks':\n            socks.add(customer)\n        if category == 'Hat':\n            hats.add(customer)\n        if category == 'Sweater':\n            sweaters.add(customer)\n\n# Print report results\nprint(\"SALES REPORT AS OF 2/2/25\\n\")\nprint(f'{len(socks)} customers bought socks.')\nprint(f'{len(socks - hats)} customers bought socks but not hats.')\nprint(f'{len(socks & hats)} customers bought socks and hats.')\nprint(f'{len(socks & sweaters)} customers bought socks and sweaters.')\nprint(f'{len(socks & hats & sweaters)} customers bought all three products.')\n\nprint(f'\\nOur {len(socks & hats & sweaters)} valued customers are:')\nfor customer in (socks & hats & sweaters):\n    print(f'  - {customer[1]}')\n\nSALES REPORT AS OF 2/2/25\n\n271 customers bought socks.\n240 customers bought socks but not hats.\n31 customers bought socks and hats.\n79 customers bought socks and sweaters.\n9 customers bought all three products.\n\nOur 9 valued customers are:\n  - Ledner-Kling\n  - Koepp-McLaughlin\n  - Kuvalis-Roberts\n  - Fritsch-Glover\n  - Mills Inc\n  - Halvorson PLC\n  - Beier-Bosco\n  - Upton, Runolfsson and O'Reilly\n  - Bashirian, Beier and Watsica\n\n\nCheck out our Polars course to upskill your data analysis skills."
  },
  {
    "objectID": "blog/my-marimo-pr/index.html",
    "href": "blog/my-marimo-pr/index.html",
    "title": "I made my first pull request to the Marimo team",
    "section": "",
    "text": "If you work extensively with data in Python, you‚Äôll agree that Jupyter notebooks provide an excellent environment for data analysis. I‚Äôve used Jupyter notebooks for a long time and love their versatility, but they come with limitations. Since there was no better alternative, I had to put up these limitations, until Marimo came into the picture."
  },
  {
    "objectID": "blog/my-marimo-pr/index.html#what-is-marimo",
    "href": "blog/my-marimo-pr/index.html#what-is-marimo",
    "title": "I made my first pull request to the Marimo team",
    "section": "What is marimo?",
    "text": "What is marimo?\nThe Marimo team describes their product as ‚Äúa next-generation Python notebook.‚Äù While this may sound like typical marketing speak, the notebook is genuinely innovative, even in its early stages. Most importantly, the Marimo notebook is reactive and interactive.\nFor me, the biggest advantage of Marimo notebooks is their dynamic nature. They address one of the most frustrating aspects of Jupyter notebooks: the need to run cells linearly. If you‚Äôve ever downloaded a Jupyter notebook from a GitHub repo, you‚Äôll know that most of them fail to run successfully due to improperly ordered code cells. Fixing this is manageable if you created the notebook, but it‚Äôs a headache if you didn‚Äôt.\nMarimo notebooks, however, do not require cells to be run in a specific order. Instead, code cells are interconnected in a network of nodes and edges. When you update one cell, all dependent cells are automatically updated. This flexibility allows you to rearrange cells however you like‚Äîan essential feature because data analysis is rarely a linear process. It‚Äôs an iterative, messy process that involves trial and error."
  },
  {
    "objectID": "blog/my-marimo-pr/index.html#the-pull-requests-i-made",
    "href": "blog/my-marimo-pr/index.html#the-pull-requests-i-made",
    "title": "I made my first pull request to the Marimo team",
    "section": "The pull requests I made",
    "text": "The pull requests I made\nI came across a post in the Polars Discord channel from the creator of Marimo:\n\n\n\n\na call to contributors\n\n\n\nAs a Polars expert, I immediately jumped on the opportunity to contribute to the Marimo team because I enjoy using their product. I contributed two notebooks focused on basic operations and aggregations. You can access them here:\n\nBasic operations notebook\nAggregations notebook\n\nSubmitting my first public pull request was a rewarding experience. I learned a lot from the feedback I received, and having a fresh pair of eyes review my code was truly enlightening.\nCheck out my Polars course on Udemy to improve your data analysis skills with this increasingly popular Python library."
  },
  {
    "objectID": "blog/kivy-desktop-app/index.html",
    "href": "blog/kivy-desktop-app/index.html",
    "title": "Creating a desktop app using kivy in python",
    "section": "",
    "text": "I love watching movies and TV shows, but there are just too many out there that it‚Äôs often difficult to pick what to watch. For years, I‚Äôve been updating my database of actresses whose performance impressed me and the movies or TV shows they appear in. Using this database, I created a desktop app uisng a Python library called Kivy. My rationale is as follows:\nThis, therefore, serves as a filter to only focus on movies or TV shows that have these actresses, thereby narrowing my search for possible movies or shows to watch."
  },
  {
    "objectID": "blog/kivy-desktop-app/index.html#how-the-app-works",
    "href": "blog/kivy-desktop-app/index.html#how-the-app-works",
    "title": "Creating a desktop app using kivy in python",
    "section": "How the app works",
    "text": "How the app works\nWhen launched, the app prompts you to enter an actress‚Äôs name or select one from a dropdown list.\n\nSelecting a name displays her image along with movies/series she has been in.\nTyping a name requires pressing Enter to display the image.\n\nHere‚Äôs the display for a successful search.\n\n\n\n\noutput from the app\n\n\n\nOr you can watch the video below where I show how the app works and explain parts of the code I wrote to create it."
  },
  {
    "objectID": "blog/kivy-desktop-app/index.html#checkout-the-source-code",
    "href": "blog/kivy-desktop-app/index.html#checkout-the-source-code",
    "title": "Creating a desktop app using kivy in python",
    "section": "Checkout the source code",
    "text": "Checkout the source code\nI‚Äôve open sourced the code for this app. You can check it out on my github repo and experiment with it. Let me know how you‚Äôve improved it to make it your own."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Conterval",
    "section": "",
    "text": "The single most important lesson I learned from my retired boss\n\n\n\n\n\n\n\n\n\n\n\n2025-05-18\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nLet me be the new host of The Data Scientist Show\n\n\n\n\n\n\n\n\n\n\n\n2025-04-26\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nI made my first pull request to the Marimo team\n\n\n\n\n\n\n\n\n\n\n\n2025-03-03\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nHow to improve a bad graph with plotly\n\n\n\n\n\n\n\n\n\n\n\n2025-02-25\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a desktop app using kivy in python\n\n\n\n\n\n\n\n\n\n\n\n2025-02-22\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nUsing set theory to speed up your data analysis\n\n\n\n\n\n\n\n\n\n\n\n2025-02-09\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nHow overlooking a small detail on a job interview can disqualify you for a position\n\n\n\n\n\n\n\n\n\n\n\n2025-02-01\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nTen polars functions that pros use and amateurs don‚Äôt\n\n\n\n\n\n\n\n\n\n\n\n2025-01-13\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nTranforming timeseries data with group by and group by dynamic in polars\n\n\n\n\n\n\n\n\n\n\n\n2025-01-06\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I learned about group by dynamic in polars while working on a client‚Äôs project\n\n\n\n\n\n\n\n\n\n\n\n2024-12-30\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create charts from The Economist magazine using plotly\n\n\n\n\n\n\n\n\n\n\n\n2024-12-15\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nWhat tool should you use as a data analyst?\n\n\n\n\n\n\n\n\n\n\n\n2024-12-01\n\n\nJoram Mutenge\n\n\n\n\n\n\n\n\n\n\n\n\nHow we helped a bakery generate forecast by bread type using polars\n\n\n\n\n\n\n\n\n\n\n\n2024-11-18\n\n\nJoram Mutenge\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html",
    "href": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html",
    "title": "How to create charts from The Economist magazine using plotly",
    "section": "",
    "text": "We at Conterval have always been fans of the charts from The Economist magazine. No publication does a better job of creating static visualizations you can use in print. We love their charts because they‚Äôre simple, yet they manage to convey the relevant message contained in the data. More importantly, they adhere to Ben Shneiderman‚Äôs point that:\n\nThe purpose of visualization is insight, not pictures.\n\nIt turns out that having too much graphic detail on your charts is not as effective at conveying the message as having a simple chart. The Economist is the master of simple, yet informative charts.\nUnfortunately, many people focus on the chart‚Äôs aesthetics‚Äîhow it looks to the human eye. In so doing, they neglect the message contained in the data, which they are supposed to convey to the audience.\nAlthough we may not know the specific software or graphing library used by The Economist for their charts, their simplicity makes them easy to recreate with any competent graphing tool. In this case, we‚Äôll recreate two of their charts using the Python graphing library Plotly."
  },
  {
    "objectID": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#introduction",
    "href": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#introduction",
    "title": "How to create charts from The Economist magazine using plotly",
    "section": "",
    "text": "We at Conterval have always been fans of the charts from The Economist magazine. No publication does a better job of creating static visualizations you can use in print. We love their charts because they‚Äôre simple, yet they manage to convey the relevant message contained in the data. More importantly, they adhere to Ben Shneiderman‚Äôs point that:\n\nThe purpose of visualization is insight, not pictures.\n\nIt turns out that having too much graphic detail on your charts is not as effective at conveying the message as having a simple chart. The Economist is the master of simple, yet informative charts.\nUnfortunately, many people focus on the chart‚Äôs aesthetics‚Äîhow it looks to the human eye. In so doing, they neglect the message contained in the data, which they are supposed to convey to the audience.\nAlthough we may not know the specific software or graphing library used by The Economist for their charts, their simplicity makes them easy to recreate with any competent graphing tool. In this case, we‚Äôll recreate two of their charts using the Python graphing library Plotly."
  },
  {
    "objectID": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#dumbbell-plot",
    "href": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#dumbbell-plot",
    "title": "How to create charts from The Economist magazine using plotly",
    "section": "Dumbbell plot",
    "text": "Dumbbell plot\nA dumbbell plot is a dot plot that uses straight lines to connect two points for each group. This type of chart is ideal for illustrating changes in a variable over two time points or highlighting the range of a variable across multiple groups. The Economist dumbbell chart below has two groups (1970 and 2020) joined by two dots. These years are points in time.\n\n\n\n\nThe Economist dumbbell chart\n\n\n\n\nSuits dataset\nSince we didn‚Äôt have the data used to create the chart above, we put together some fake department data about a company from one of our favorite TV shows, Suits.\nBelow is a dataframe showing customer satisfaction scores for each department of the fictional firm Pearson Specter Litt for the years 2023 and 2024.\n\n\n\nshape: (8, 3)\n\n\n\nDepartment\n2023\n2024\n\n\nstr\ni64\ni64\n\n\n\n\n\"Customer Service\"\n80\n70\n\n\n\"Finance\"\n60\n70\n\n\n\"HR\"\n75\n80\n\n\n\"IT\"\n45\n55\n\n\n\"Operations\"\n85\n78\n\n\n\"Procurement\"\n32\n55\n\n\n\"Production\"\n20\n35\n\n\n\"Sales\"\n60\n65\n\n\n\n\n\n\n\nAnd now here‚Äôs a Plotly recreation of our dumbbell plot.\n\nBG_COLOR = '#E9EDF0'\nRED = '#E3120B'\nGREY = '#5e5c64'\n\nfig = go.Figure()\n\nfor i in range(df.shape[0]):\n    fig.add_trace(go.Scatter(\n        x=[df[\"2023\"][i], df[\"2024\"][i]],\n        y=[df[\"Department\"][i], df[\"Department\"][i]],\n        mode='lines+markers',\n        line=dict(color='#598080', width=4),  # Thicker line\n        marker=dict(size=16),  # Bigger dots\n        showlegend=False\n    ))\n    fig.add_trace(go.Scatter(\n        x=[df[\"2023\"][i]],\n        y=[df[\"Department\"][i]],\n        mode='markers',\n        marker=dict(color=GREY, size=16),  # Bigger dots\n        showlegend=False\n    ))\n    fig.add_trace(go.Scatter(\n        x=[df[\"2024\"][i]],\n        y=[df[\"Department\"][i]],\n        mode='markers',\n        marker=dict(color=RED, size=16),  # Bigger dots\n        showlegend=False\n    ))\n\n# Update layout to customize appearance\nfig.update_layout(\n    title=\"&lt;b&gt;Engagement score declined in&lt;br&gt;customer service & operations\",\n    title_font=dict(size=26),\n    title_y=.9,\n    plot_bgcolor=BG_COLOR,\n    paper_bgcolor=BG_COLOR,\n    height=600,\n    margin=dict(t=180, b=80),\n    xaxis=dict(\n        side=\"top\",  # Move x-axis to the top\n        tickfont=dict(size=18)  # Increase the font size of the x-axis ticks\n    ),\n    yaxis=dict(\n        tickfont=dict(size=16)\n    ),\n    shapes=[\n        dict(\n            type=\"line\",\n            xref=\"paper\",\n            yref=\"paper\",\n            x0=-0.096, y0=1.5,  \n            x1=0.06, y1=1.5,  \n            line=dict(\n                color=RED,  \n                width=10  \n            )\n        )]\n)\n\nfig.add_annotation(\n    dict(\n        text=\"&lt;b&gt;2023\",\n        x=0.1,  # x position (0 means far left)\n        y=0.72,  # y position (adjust as necessary)\n        xref=\"paper\",\n        yref=\"paper\",\n        showarrow=False,  # No arrow\n        font=dict(\n            size=22,  # Font size\n            color=GREY  # Font color\n        ),\n        align=\"left\"\n    ),\n)\n\nfig.add_annotation(\n    dict(\n        text=\"&lt;b&gt;2024\",\n        x=0.6,  # x position (0 means far left)\n        y=0.72,  # y position (adjust as necessary)\n        xref=\"paper\",\n        yref=\"paper\",\n        showarrow=False,  # No arrow\n        font=dict(\n            size=22,  # Font size\n            color=RED  # Font color\n        ),\n        align=\"left\"\n    ),\n)\n\nfig.add_layout_image(\n    dict(\n        source=f\"data:image/png;base64,{encoded_image}\",\n        xref=\"paper\",\n        yref=\"paper\",\n        x=.94,\n        y=-0.18,\n        xanchor=\"right\",\n        yanchor=\"bottom\",\n        sizex=0.22,\n        sizey=0.22,\n        layer=\"above\"\n    )\n)\n\nfig.add_annotation(\n    dict(\n        text=\"Source: Pearson Specter Litt\",\n        x=-0.14,  # x position (0 means far left)\n        y=-0.175,  # y position (adjust as necessary)\n        xref=\"paper\",\n        yref=\"paper\",\n        showarrow=False,  # No arrow\n        font=dict(\n            size=14,  # Font size\n            color=GREY  # Font color\n        ),\n        align=\"left\"\n    ),\n)\n\nfig.show(renderer=\"iframe\")\n\n\n\n\n\nDisregard the differences between the data in my chart and The Economist‚Äôs chart, and focus on the style. You‚Äôll see that the styles are identical. This highlights Plotly‚Äôs versatility as a graphing library‚Äîallowing you to fully customize any chart to match your preferences."
  },
  {
    "objectID": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#stacked-bar-plot",
    "href": "blog/how-to-create-charts-from-the-economist-magazine-using-plotly/index.html#stacked-bar-plot",
    "title": "How to create charts from The Economist magazine using plotly",
    "section": "Stacked bar plot",
    "text": "Stacked bar plot\nA stacked bar chart is a type of bar graph where each bar is divided into segments that represent different subcategories. It shows how each subcategory contributes to the total value of a larger category, making it easy to see the breakdown of the whole.\n\n\n\n\nThe Economist stacked bar chart\n\n\n\nWhat makes this chart unique is the placement of the major group names above each bar, rather than beside them. This design choice offers two key advantages: it makes it easier to associate each category with its corresponding bar and saves space, resulting in a more compact plot. Additionally, the data varies significantly‚Äîfor example, from 11 to 998‚Äîwhich would distort the scale in a regular bar chart. By opting for a stacked bar chart, The Economist effectively addressed this issue, demonstrating thoughtful and intentional design.\n\nMilitary dataset\nFortunately, all the data needed to recreate the The Economist stacked bar plot is visible in the plot. Here‚Äôs a dataframe of that data showing the category of military technology for the United States and China.\n\n\n\nshape: (5, 3)\n\n\n\nCategory\nChina\nUnited States\n\n\nstr\ni64\ni64\n\n\n\n\n\"Total battle force\"\n370\n291\n\n\n\"Principle surface combatants\"\n92\n122\n\n\n\"Aircraft-carriers\"\n2\n11\n\n\n\"Combat aircaft\"\n456\n988\n\n\n\"Helicopters\"\n116\n689\n\n\n\n\n\n\n\nAnd now here‚Äôs a Plotly recreation of our stacked bar plot.\n\ndf = (data\n .with_columns(China_pct=pl.col('China') / pl.sum_horizontal('China','United States'),\n               US_pct=pl.col('United States') / pl.sum_horizontal('China','United States'),\n               Category=pl.concat_str([pl.lit('&lt;b&gt;'), pl.col('Category')]))\n )\n\n# Create the percentage stacked bar chart using Plotly Graph Objects\nimport plotly.graph_objects as go\n\n# Adding China's df as percentage with left-aligned text\nfig = go.Figure()\n\n# China's df with actual number label\nfig.add_trace(go.Bar(\n    y=[1, 2, 3, 4, 5],  # Use a numerical y-axis instead of category labels\n    x=df['China_pct'],\n    width=.5,\n    name='China',\n    marker=dict(color='#E3120B'),\n    orientation='h',\n    text=df['China'],  # Actual number label\n    textposition='inside',  # Place text inside the bar\n    insidetextanchor='start'  # Left-align the text inside the bars\n))\n\n# United States' df with actual number label\nfig.add_trace(go.Bar(\n    y=[1, 2, 3, 4, 5],  # Match the same y-values for the US df\n    x=df['US_pct'],\n    name='United States',\n    marker=dict(color='#9a9996'),  # Set the color of the bars\n    width=.5,\n    orientation='h',\n    text=df['United States'],  # Actual number label\n    textposition='inside',  # Place text inside the bar\n    insidetextanchor='end'  # Right-align the text inside the bars\n))\n\n# Adding annotations for the category labels above the bars\nfor i, category in enumerate(df['Category']):\n    fig.add_annotation(\n        x=0,  # Align annotation at the start (left)\n        y=i+1.45,  # Adjust the y value to move the annotation slightly above the bar\n        text=category,  # The category label\n        showarrow=False,  # Disable arrows\n        xanchor='left',  # Left-align the annotation\n        yanchor='middle',  # Center the annotation vertically\n        xshift=0  # No horizontal shift\n    )\n\n# Update layout for percentage stacking and legend positioning\nfig.update_layout(\n    height=600,\n    margin=dict(t=200, b=50),\n    barmode='stack',\n    title=\"&lt;b&gt;China's military tech&lt;/b&gt;&lt;br&gt;&lt;b&gt;still lags the West&lt;/b&gt;\",\n    title_y=.9,\n    title_x=0.023,\n    title_font=dict(size=26),\n    # xaxis_title='Percentage of Total Production',\n    yaxis_title='',  # Remove the category labels from y-axis\n    xaxis=dict(\n        tickvals=[0, 25, 50, 75, 100],  # Set tick marks for percentage\n        showgrid=False,  # Hide grid lines\n        showticklabels=False  # Hide tick labels\n    ),\n    shapes=[\n        dict(\n            type=\"line\",\n            xref=\"paper\",\n            yref=\"paper\",\n            x0=0, y0=1.52,  \n            x1=0.1, y1=1.52,  \n            line=dict(\n                color=RED,  \n                width=10  \n            )\n        )],\n    # Consolidating yaxis settings here\n    yaxis=dict(\n        showticklabels=False,  # Hide the default y-axis labels\n        showgrid=False,\n        tickvals=[1, 2, 3, 4, 5],  # Keep the numerical y-axis values\n        ticktext=['', '', '', '', ''],  # Hide y-axis text to avoid overlap\n        range=[0.5, 5.5]  # Adjust the range to add some padding\n    ),\n    \n    # Adjust the gap between the bars\n    bargap=0.65,  # Increase this to widen the gap between bars\n\n    # Adjust legend positioning\n    legend=dict(\n        x=.96,        # Horizontal position (1 is far right)\n        y=1.04,      # Vertical position (1 is top)\n        xanchor='right',  # Anchors the legend box to the right side of the plot\n        yanchor='top',    # Anchors the legend box to the top of the plot\n        orientation='h'   # Set the legend items to be horizontal\n    ),\n    \n    plot_bgcolor='#E9EDF0',\n    paper_bgcolor='#E9EDF0',\n\n    # Set the font to INTER\n    font=dict(\n        size=12,  # Adjust the size as needed\n        color=\"Black\"  # Adjust the color as needed\n    ),\n)\n\nfig.add_annotation(\n    dict(\n           text=\"Sources: IISS; US Department of Defence\",\n           x=-0.005,  # x position (0 means far left)\n           y=-0.12,  # y position (adjust as necessary)\n           xref=\"paper\",\n           yref=\"paper\",\n           showarrow=False,  # No arrow\n           font=dict(\n               # size=12,  # Font size\n               color=\"#9a9996\"  # Font color\n           ),\n           align=\"left\"\n       ),\n)\n\nfig.add_annotation(\n    dict(\n           text=\"&lt;b&gt; Navy balance, December 2022 or latest available\",\n           x=-0.01,  # x position (0 means far left)\n           y=1.2,  # y position (adjust as necessary)\n           xref=\"paper\",\n           yref=\"paper\",\n           showarrow=False,  # No arrow\n           font=dict(\n               size=18,  # Font size\n               color=\"#1a1a1a\"  # Font color\n           ),\n           align=\"left\"\n       ),\n)\n\nfig.add_layout_image(\n    dict(\n        source=f\"data:image/png;base64,{encoded_image}\",\n        xref=\"paper\",  # Reference the x position relative to the plotting area\n        yref=\"paper\",  # Reference the y position relative to the plotting area\n        x=.95,  # x-coordinate (1 means far right)\n        y=-0.12,  # y-coordinate (0 means bottom)\n        xanchor=\"right\",  # Anchor the image from the right\n        yanchor=\"bottom\",  # Anchor the image from the bottom\n        sizex=0.2,  # Set the width of the image (adjust as necessary)\n        sizey=0.2,  # Set the height of the image (adjust as necessary)\n        layer=\"above\"  # Make sure the image is displayed above the plot\n    )\n)\n\nfig.show(renderer=\"iframe\")\n\n\n\n\n\nAnd there you have it! We‚Äôve successfully recreated charts inspired by The Economist magazine using Plotly for graphing and Polars for data manipulation.\nIf you need custom plots for your data, whether for print or digital media, feel free to reach out to us!"
  },
  {
    "objectID": "blog/data-scientist-show/index.html",
    "href": "blog/data-scientist-show/index.html",
    "title": "Let me be the new host of The Data Scientist Show",
    "section": "",
    "text": "I messaged Daliana Liu, host of The Data Scientist Show, on LinkedIn asking to become the new host. If you know her, please share this post so she sees it.\nHere‚Äôs the message I sent:\n\nDaliana,\nI just listened to the last two episodes of The Data Scientist Show and was sad to hear you‚Äôre planning to end it. You shouldn‚Äôt. Here‚Äôs why:\n\nThis podcast has taught me, and many others, so much about the data field. It‚Äôs a valuable resource that deserves to continue, even if it means bringing in a new host.\nI respect your shift toward new interests and a new podcast, but I believe The Data Scientist Show should live on separately.\n\nI‚Äôd love to take over as host. Here‚Äôs the link to a sample recording so you can hear how I‚Äôd carry it forward.\nLet me know you thoughts.\nJoram\n\nAnd here‚Äôs the sample recording.\n\n\nYour browser does not support the audio element."
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "",
    "text": "A few weeks ago, Conterval did a consulting gig for a medium-sized bakery. This bakery makes white and brown bread, which it sells to a major retail store here in the USA. The bakery contacted our company to help clean up their forecast data and generate a forecast for each bread type.\nThe gig turned out to be an interesting experience, so we asked the bakery if I could write about the experience on the company blog, and they said yes. In this post, I‚Äôll share what the bakery‚Äôs problem was and the solution we devised to solve it."
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#problem",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#problem",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Problem",
    "text": "Problem\nThe bakery receives an Excel file with forecast data from a major US retail store every week. This file contains 2 columns: Date (the 1st of every month from January to December) and Forecast (the number of loaves of bread they want in that month).\nThe challenge was that the retail store did not provide a separate forecast value for white and brown bread. The retail store just provided a single forecast value. It was up to the bakery to divide that number into how many loaves of white or brown bread to make. It turns out this was a challenging task."
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#solution",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#solution",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Solution",
    "text": "Solution\nCreate a systematic process that determines how many loaves of bread should be made for each bread type based on the provided forecast value for that month. This information should be presented in an easy to understand table.\n\n\n\n\n\n\nNote\n\n\n\nThe generated table should be easy to update based on the new forecast data provided by the retail store."
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#dataset",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#dataset",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Dataset",
    "text": "Dataset\nWe‚Äôll not use the actual data from the bakery, rather we‚Äôll use fictional data to demonstrate the solution. Here‚Äôs the baker‚Äôs sales data from last year.\n\n\n\nshape: (24, 3)\n\n\n\nDate\nBread\nSales\n\n\ndate\nstr\ni64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\n\n\n2023-01-01\n\"White\"\n203\n\n\n2023-02-01\n\"Brown\"\n329\n\n\n2023-02-01\n\"White\"\n304\n\n\n2023-03-01\n\"Brown\"\n201\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2023-10-01\n\"White\"\n425\n\n\n2023-11-01\n\"Brown\"\n383\n\n\n2023-11-01\n\"White\"\n297\n\n\n2023-12-01\n\"Brown\"\n248\n\n\n2023-12-01\n\"White\"\n200\n\n\n\n\n\n\n\nHere‚Äôs the forecast data from the retail store.\n\n\n\nshape: (12, 2)\n\n\n\nDate\nForecast\n\n\ndate\ni64\n\n\n\n\n2024-01-01\n897\n\n\n2024-02-01\n945\n\n\n2024-03-01\n865\n\n\n2024-04-01\n754\n\n\n2024-05-01\n1010\n\n\n‚Ä¶\n‚Ä¶\n\n\n2024-08-01\n777\n\n\n2024-09-01\n922\n\n\n2024-10-01\n848\n\n\n2024-11-01\n1002\n\n\n2024-12-01\n831"
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#the-math",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#the-math",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "The math",
    "text": "The math\nDetermining how many brown or white loaves of bread to make was not as easy as dividing the forecast value by 2. Why? Because in some months, the retail store buys more white bread than brown bread. In other months, it‚Äôs the reverse.\nWe decided to leverage some timeseries calculations by doing a rolling sum with a 3-months window of last year‚Äôs sales by bread type. The idea was to get the weight or percentage for each bread type and use that to determine the number of loaves to make from the forecast value.\nThis math is quite involving, but it‚Äôs easy to follow along with the data."
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#implementing-the-math",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#implementing-the-math",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Implementing the math",
    "text": "Implementing the math\nHere are the formulas for calculating the rolling sum for the rows of each bread type.\n\\[\\text{Row 1} = \\text{Jan} + \\text{Feb} + \\text{Mar}\\]\n\\[\\text{Row 2} = \\text{Feb} + \\text{Mar} + \\text{Apr}\\]\n\\[\\text{...}\\]\n\\[\\text{Second Last Row} = \\text{Nov} + \\text{Dec}\\]\n\\[\\text{Last Row} = \\text{Dec}\\]\nAnd here is a visualization showing the calculated rolling sum values for each row of brown bread.\n\n\n\n\n\ngraph TD\n    classDef sumStyle fill:#FFE4B5,stroke:#333,stroke-width:2px;\n\n    sum1[\"342 + 329 + 201 = 872\"]:::sumStyle --&gt; sum2[\"329 + 201 + 203 = 733\"]:::sumStyle\n    sum2 --&gt; sum3[\"201 + 203 + 300 = 704\"]:::sumStyle\n    sum3 --&gt; sum4[\"203 + 300 + 473 = 976\"]:::sumStyle\n    sum4 --&gt; sum5[\"300 + 473 + 287 = 1060\"]:::sumStyle\n    sum5 --&gt; sum6[\"473 + 287 + 446 = 1206\"]:::sumStyle\n    sum6 --&gt; sum7[\"287 + 446 + 305 = 1038\"]:::sumStyle\n    sum7 --&gt; sum8[\"446 + 305 + 253 = 1004\"]:::sumStyle\n    sum8 --&gt; sum9[\"305 + 253 + 383 = 941\"]:::sumStyle\n    sum9 --&gt; sum10[\"253 + 383 + 248 = 884\"]:::sumStyle\n    sum10 --&gt; sum11[\"383 + 248 = 631\"]:::sumStyle\n    sum11 --&gt; sum12[\"248\"]:::sumStyle\n\n\n\n\n\n\nNow that we know what values to expect, we can implement the math. Initially, we thought that doing a rolling_sum polars function would perform the calculation shown above but it didn‚Äôt, at least not entirely.\nLet‚Äôs demonstrate this calculation to see where it fell short.\n\n(sales_df\n .filter(pl.col('Bread').eq('Brown'))\n .with_columns(Rol_3month=pl.col('Sales').rolling_sum(window_size=3))\n )\n\n\nshape: (12, 4)\n\n\n\nDate\nBread\nSales\nRol_3month\n\n\ndate\nstr\ni64\ni64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\nnull\n\n\n2023-02-01\n\"Brown\"\n329\nnull\n\n\n2023-03-01\n\"Brown\"\n201\n872\n\n\n2023-04-01\n\"Brown\"\n203\n733\n\n\n2023-05-01\n\"Brown\"\n300\n704\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2023-08-01\n\"Brown\"\n446\n1206\n\n\n2023-09-01\n\"Brown\"\n305\n1038\n\n\n2023-10-01\n\"Brown\"\n253\n1004\n\n\n2023-11-01\n\"Brown\"\n383\n941\n\n\n2023-12-01\n\"Brown\"\n248\n884\n\n\n\n\n\n\n This gives us some of the values we want, but it creates null values for the first 2 rows. To rectify the null value problem on the first two rows, we shifted the values in Rol_3month up by 2 rows.\n\n(sales_df\n .filter(pl.col('Bread').eq('Brown'))\n .with_columns(Rol_3month=pl.col('Sales').rolling_sum(window_size=3))\n .with_columns(Rol_3month_Shift=pl.col('Sales').rolling_sum(window_size=3).shift(-2))\n )\n\n\nshape: (12, 5)\n\n\n\nDate\nBread\nSales\nRol_3month\nRol_3month_Shift\n\n\ndate\nstr\ni64\ni64\ni64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\nnull\n872\n\n\n2023-02-01\n\"Brown\"\n329\nnull\n733\n\n\n2023-03-01\n\"Brown\"\n201\n872\n704\n\n\n2023-04-01\n\"Brown\"\n203\n733\n976\n\n\n2023-05-01\n\"Brown\"\n300\n704\n1060\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2023-08-01\n\"Brown\"\n446\n1206\n1004\n\n\n2023-09-01\n\"Brown\"\n305\n1038\n941\n\n\n2023-10-01\n\"Brown\"\n253\n1004\n884\n\n\n2023-11-01\n\"Brown\"\n383\n941\nnull\n\n\n2023-12-01\n\"Brown\"\n248\n884\nnull\n\n\n\n\n\n\n The problem of null values in the first 2 rows is solved, but another problem is created. The last 2 rows now have null values. At this point, we knew that rolling_sum wasn‚Äôt going to work.\nThe reason why rolling sum didn‚Äôt work is that on the first row, we don‚Äôt yet have 3 values to add so the sum is null, the same applies to the second row. But on the second row, we have 3 values in the window to add that‚Äôs why the first value shows up on row 3.\nBut since we wanted the value on row 3 to be on the first row, we shifted the values up by 2 rows, but that only created null values on the bottom two rows. Also, since the rolling sum shifts down one row to get the next 3 values, eventually there won‚Äôt be enough 3 values to add. That‚Äôs why we have null values.\nHowever, from the formulas above, we see that if there are no 3 values to add, the rolling sum calculation proceeds by calculating the available values. So for the second to last row, it‚Äôs only 2 values (Nov + Dec), and for the last row, it‚Äôs only 1 value (Dec)."
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#successful-implementation-of-solution",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#successful-implementation-of-solution",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Successful implementation of solution",
    "text": "Successful implementation of solution\nThe solution that worked involved the use of implode. Here‚Äôs how we implemented it. Let‚Äôs focus on brown bread only so we can see that the rolling sum values in the dataframe are the same as those in the visualization.\nWe‚Äôll begin by creating a list of all the dates in the sales data.\n\ndate_list = sales_df['Date'].unique().to_list()\ndate_list\n\n[datetime.date(2023, 1, 1),\n datetime.date(2023, 2, 1),\n datetime.date(2023, 3, 1),\n datetime.date(2023, 4, 1),\n datetime.date(2023, 5, 1),\n datetime.date(2023, 6, 1),\n datetime.date(2023, 7, 1),\n datetime.date(2023, 8, 1),\n datetime.date(2023, 9, 1),\n datetime.date(2023, 10, 1),\n datetime.date(2023, 11, 1),\n datetime.date(2023, 12, 1)]\n\n\n Now, let‚Äôs write some code to calculate the Rol_3month value for the first date in date_list. This date value will be accessed with 0 index as in date_list[0].\n\n(sales_df\n .filter(pl.col('Bread').eq('Brown'))\n .with_columns(Sales_List=pl.col('Sales').implode())\n .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n .filter(pl.col('Date').eq(date_list[0]))\n)\n\n\nshape: (1, 5)\n\n\n\nDate\nBread\nSales\nSales_List\nRol_3month\n\n\ndate\nstr\ni64\nlist[i64]\ni64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\n[342, 329, ‚Ä¶ 248]\n872\n\n\n\n\n\n\n In the code above, we filtered the data to only show brown bread, then created a column Sales_List using implode. This stores all sales values from January to December into a single list. To calculate the Rol_3month, we slice the list of sales values to only select the first available 3 values and then add them up. Finally, we only get the row in the dataframe that corresponds to the chosen date, which is the first date in date_list.\nLet‚Äôs reuse this code to calculate the Rol_3month value for white bread. Because we want to have a single dataframe showing the results for brown and white bread, we‚Äôll use vstack to vertically combine the dataframes.\n\n(sales_df\n .filter(pl.col('Bread').eq('Brown'))\n .with_columns(Sales_List=pl.col('Sales').implode())\n .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n .filter(pl.col('Date').eq(date_list[0]))\n .vstack(sales_df\n .filter(pl.col('Bread').eq('White'))\n .with_columns(Sales_List=pl.col('Sales').implode())\n .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n .filter(pl.col('Date').eq(date_list[0])))\n)\n\n\nshape: (2, 5)\n\n\n\nDate\nBread\nSales\nSales_List\nRol_3month\n\n\ndate\nstr\ni64\nlist[i64]\ni64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\n[342, 329, ‚Ä¶ 248]\n872\n\n\n2023-01-01\n\"White\"\n203\n[203, 304, ‚Ä¶ 200]\n880"
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#calculating-percentages",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#calculating-percentages",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Calculating percentages",
    "text": "Calculating percentages\nHere‚Äôs the formula we used to calculate the percentage or weight for each bread type. Let‚Äôs focus on brown bread for the month of January.\n\\[\\% \\text{ of Brown Bread} = \\frac{\\text{January Rol\\_3month}}{\\text{January Rol\\_3month} + \\text{White Bread January Rol\\_3month}}\\]\nWe are dividing each Rol_3month value for every bread type by the sum of the Rol_3month values for both bread types. Let‚Äôs put this into code. Also, we don‚Äôt need Sales_List, so we‚Äôll drop it.\n\n(sales_df\n .filter(pl.col('Bread').eq('Brown'))\n .with_columns(Sales_List=pl.col('Sales').implode())\n .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n .filter(pl.col('Date').eq(date_list[0]))\n .drop('Sales_List')\n .vstack(sales_df\n .filter(pl.col('Bread').eq('White'))\n .with_columns(Sales_List=pl.col('Sales').implode())\n .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n .filter(pl.col('Date').eq(date_list[0]))\n .drop('Sales_List'))\n .with_columns(Percentage=pl.col('Rol_3month') / pl.col('Rol_3month').sum())\n )\n\n\nshape: (2, 5)\n\n\n\nDate\nBread\nSales\nRol_3month\nPercentage\n\n\ndate\nstr\ni64\ni64\nf64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\n872\n0.497717\n\n\n2023-01-01\n\"White\"\n203\n880\n0.502283\n\n\n\n\n\n\n\nThese are the values we want, but we‚Äôve only calculated for the first date in date_list. We have to perform this calculation for every date in date_list. Rather than doing it manually 12 times, we‚Äôll use a for loop to loop through the date_list.\nIt turns out that looping doesn‚Äôt work on a list of dates, so we‚Äôll create a list of 12 numbers from 0 to 11. These numbers will be used as indices to represent each date item in date_list. Thus, to use the first date in the list, we use date_list[0].\nLet‚Äôs create a list of the numbers and store them in a variable called num_list.\n\nnum_list = list(range(len(date_list)))\nnum_list\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\n\nNow let‚Äôs implement the for loop to get the desired dataframe.\n\npct_dfs = []\nfor i in num_list:\n    pct_df = (sales_df\n              .filter(pl.col('Bread').eq('Brown'))\n              .with_columns(Sales_List=pl.col('Sales').implode())\n              .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n              .filter(pl.col('Date').eq(date_list[i]))\n              .drop('Sales_List')\n              .vstack(sales_df\n              .filter(pl.col('Bread').eq('White'))\n              .with_columns(Sales_List=pl.col('Sales').implode())\n              .with_columns(Rol_3month=pl.col('Sales_List').list.slice(0, 3).list.sum())\n              .filter(pl.col('Date').eq(date_list[i]))\n              .drop('Sales_List'))\n              .with_columns(Percentage=pl.col('Rol_3month') / pl.col('Rol_3month').sum())\n              )\n    pct_dfs.append(pct_df)\ndf_with_pct = pl.concat(pct_dfs)\ndf_with_pct\n\n\nshape: (24, 5)\n\n\n\nDate\nBread\nSales\nRol_3month\nPercentage\n\n\ndate\nstr\ni64\ni64\nf64\n\n\n\n\n2023-01-01\n\"Brown\"\n342\n872\n0.497717\n\n\n2023-01-01\n\"White\"\n203\n880\n0.502283\n\n\n2023-02-01\n\"Brown\"\n329\n872\n0.497717\n\n\n2023-02-01\n\"White\"\n304\n880\n0.502283\n\n\n2023-03-01\n\"Brown\"\n201\n872\n0.497717\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2023-10-01\n\"White\"\n425\n880\n0.502283\n\n\n2023-11-01\n\"Brown\"\n383\n872\n0.497717\n\n\n2023-11-01\n\"White\"\n297\n880\n0.502283\n\n\n2023-12-01\n\"Brown\"\n248\n872\n0.497717\n\n\n2023-12-01\n\"White\"\n200\n880\n0.502283"
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#joining-forecast-data",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#joining-forecast-data",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Joining forecast data",
    "text": "Joining forecast data\nTo get the forecast values for each bread type based on percentage values, we must join our dataframe with the forecast data provided by the retail store. We‚Äôll join the dataframes on Date column.\n\n\n\n\n\n\nNote\n\n\n\nOur sales data has the year 2023 while the forecast data has the year 2024. This means we won‚Äôt be able to join. We have to modify the dates so they match.\n\n\nTo make the dates in both dataframes match, we‚Äôll remove the year in the date value. Below is the code that removes the year in the date value for the sales data.\n\n(df_with_pct\n .with_columns(pl.col('Date').dt.strftime('%m-%d'))\n)\n\n\nshape: (24, 5)\n\n\n\nDate\nBread\nSales\nRol_3month\nPercentage\n\n\nstr\nstr\ni64\ni64\nf64\n\n\n\n\n\"01-01\"\n\"Brown\"\n342\n872\n0.497717\n\n\n\"01-01\"\n\"White\"\n203\n880\n0.502283\n\n\n\"02-01\"\n\"Brown\"\n329\n872\n0.497717\n\n\n\"02-01\"\n\"White\"\n304\n880\n0.502283\n\n\n\"03-01\"\n\"Brown\"\n201\n872\n0.497717\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"10-01\"\n\"White\"\n425\n880\n0.502283\n\n\n\"11-01\"\n\"Brown\"\n383\n872\n0.497717\n\n\n\"11-01\"\n\"White\"\n297\n880\n0.502283\n\n\n\"12-01\"\n\"Brown\"\n248\n872\n0.497717\n\n\n\"12-01\"\n\"White\"\n200\n880\n0.502283\n\n\n\n\n\n\n\nIn the code below, we remove the year in the forecast data and join the two dataframes in a single dataframe called combined_df.\n\ncombined_df = (df_with_pct\n .with_columns(pl.col('Date').dt.strftime('%m-%d'))\n .join(forecast_df.with_columns(pl.col('Date').dt.strftime('%m-%d')),\n       on='Date', how='left')\n )\ncombined_df\n\n\nshape: (24, 6)\n\n\n\nDate\nBread\nSales\nRol_3month\nPercentage\nForecast\n\n\nstr\nstr\ni64\ni64\nf64\ni64\n\n\n\n\n\"01-01\"\n\"Brown\"\n342\n872\n0.497717\n897\n\n\n\"01-01\"\n\"White\"\n203\n880\n0.502283\n897\n\n\n\"02-01\"\n\"Brown\"\n329\n872\n0.497717\n945\n\n\n\"02-01\"\n\"White\"\n304\n880\n0.502283\n945\n\n\n\"03-01\"\n\"Brown\"\n201\n872\n0.497717\n865\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"10-01\"\n\"White\"\n425\n880\n0.502283\n848\n\n\n\"11-01\"\n\"Brown\"\n383\n872\n0.497717\n1002\n\n\n\"11-01\"\n\"White\"\n297\n880\n0.502283\n1002\n\n\n\"12-01\"\n\"Brown\"\n248\n872\n0.497717\n831\n\n\n\"12-01\"\n\"White\"\n200\n880\n0.502283\n831"
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#calculating-new-forecast",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#calculating-new-forecast",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Calculating new forecast",
    "text": "Calculating new forecast\nNow we have all the data needed to calculate the forecast for each bread type. All the forecast values are rounded to the nearest whole number. After all, you cannot make 1.67 loaves of bread!\n\nnew_fcst_df = (combined_df\n .with_columns(New_Forecast=(pl.col('Percentage') * pl.col('Forecast')).round().cast(pl.Int16))\n .with_columns(pl.col('Date').add(pl.lit('-2024')).str.strptime(pl.Date, \"%m-%d-%Y\"))\n )\nnew_fcst_df\n\n\nshape: (24, 7)\n\n\n\nDate\nBread\nSales\nRol_3month\nPercentage\nForecast\nNew_Forecast\n\n\ndate\nstr\ni64\ni64\nf64\ni64\ni16\n\n\n\n\n2024-01-01\n\"Brown\"\n342\n872\n0.497717\n897\n446\n\n\n2024-01-01\n\"White\"\n203\n880\n0.502283\n897\n451\n\n\n2024-02-01\n\"Brown\"\n329\n872\n0.497717\n945\n470\n\n\n2024-02-01\n\"White\"\n304\n880\n0.502283\n945\n475\n\n\n2024-03-01\n\"Brown\"\n201\n872\n0.497717\n865\n431\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2024-10-01\n\"White\"\n425\n880\n0.502283\n848\n426\n\n\n2024-11-01\n\"Brown\"\n383\n872\n0.497717\n1002\n499\n\n\n2024-11-01\n\"White\"\n297\n880\n0.502283\n1002\n503\n\n\n2024-12-01\n\"Brown\"\n248\n872\n0.497717\n831\n414\n\n\n2024-12-01\n\"White\"\n200\n880\n0.502283\n831\n417"
  },
  {
    "objectID": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#final-output",
    "href": "blog/helping-bakery-generate-forecast-by-bread-type-using-polars/index.html#final-output",
    "title": "How we helped a bakery generate forecast by bread type using polars",
    "section": "Final output",
    "text": "Final output\nWe now have the forecast values for each bread type, but we must present the data in a format that is not only human-readable but also easy to understand. We‚Äôll select the relevant columns and transform the data into the desired format.\n\ntable_df = (new_fcst_df\n .select('Date','Bread','New_Forecast')\n .with_columns(pl.col('Date').dt.strftime('%b'))\n .pivot(on='Date', index='Bread')\n )\ntable_df\n\n\nshape: (2, 13)\n\n\n\nBread\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nstr\ni16\ni16\ni16\ni16\ni16\ni16\ni16\ni16\ni16\ni16\ni16\ni16\n\n\n\n\n\"Brown\"\n446\n470\n431\n375\n503\n373\n478\n387\n459\n422\n499\n414\n\n\n\"White\"\n451\n475\n434\n379\n507\n377\n482\n390\n463\n426\n503\n417\n\n\n\n\n\n\n\nThis format is better, but since at Conterval we‚Äôre sticklers for aesthetics, we decided to make the final forecast output look even better. To do this, we used a library called great-tables.\n\nfrom great_tables import GT, style, loc, google_font, html\n\nmonth_list = table_df.columns[1:]\ncol_spacing = {month: '60px' for month in month_list}\n\n(\n    GT(table_df, rowname_col=\"Bread\")\n    .tab_stubhead(label=html('&lt;b&gt;Bread'))\n    .tab_header(title=html(\"&lt;h2&gt;Bread Types Forecast 2024&lt;/h2&gt;\"))\n    .tab_options(\n        table_background_color='#ffbe6f',\n        row_group_font_weight='bold',\n        quarto_disable_processing=True\n    )\n    .tab_style(\n        style=style.text(weight='bold', font=google_font(name=\"Fjalla One\")),\n        locations=loc.column_header()\n    )\n    .cols_width(cases=col_spacing)\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Bread Types Forecast 2024\n  \n\n  Bread\n  Jan\n  Feb\n  Mar\n  Apr\n  May\n  Jun\n  Jul\n  Aug\n  Sep\n  Oct\n  Nov\n  Dec\n\n\n\n  \n    Brown\n    446\n    470\n    431\n    375\n    503\n    373\n    478\n    387\n    459\n    422\n    499\n    414\n  \n  \n    White\n    451\n    475\n    434\n    379\n    507\n    377\n    482\n    390\n    463\n    426\n    503\n    417\n  \n\n\n\n\n\n\n\n\n\nContact us for help with your data problems. Also check out our Polars course to level up your data analysis skills with this fast Python library."
  },
  {
    "objectID": "blog/improve-chart/index.html",
    "href": "blog/improve-chart/index.html",
    "title": "How to improve a bad graph with plotly",
    "section": "",
    "text": "All data visualizations should, first and foremost, inform. Any visualization that falls short of this is simply data art. Data visualizations that are uninformative may be bad but misleading ones are the worst. Here‚Äôs an example of a bad visualization that Olga Berezovsky posted on LinkedIn. I thought I‚Äôd recreate it based on the suggestions she highlighted."
  },
  {
    "objectID": "blog/improve-chart/index.html#the-dataset",
    "href": "blog/improve-chart/index.html#the-dataset",
    "title": "How to improve a bad graph with plotly",
    "section": "The dataset",
    "text": "The dataset\nBelow is a polars dataframe showing the data I‚Äôll use to recreate the bad graph. This is the original data as it‚Äôs shown on the bad graph.\n\n\n\nshape: (10, 2)\n\n\n\nCountry\nGDP\n\n\nstr\ni64\n\n\n\n\n\"USA\"\n22670\n\n\n\"China\"\n16640\n\n\n\"Japan\"\n5370\n\n\n\"Germany\"\n4310\n\n\n\"UK\"\n3120\n\n\n\"India\"\n3040\n\n\n\"France\"\n2930\n\n\n\"Italy\"\n2100\n\n\n\"Canada\"\n1880\n\n\n\"Korea\"\n1800\n\n\n\n\n\n\n\nSince the GDP numbers will be presented in trillion and not billion, I‚Äôll divide the values by 1,000 and round the values to 2 decimal places. Here‚Äôs the resulting dataframe.\n\ndf = (data\n      .with_columns((pl.col('GDP') / 1000).round(2))\n     )\ndf\n\n\nshape: (10, 2)\n\n\n\nCountry\nGDP\n\n\nstr\nf64\n\n\n\n\n\"USA\"\n22.67\n\n\n\"China\"\n16.64\n\n\n\"Japan\"\n5.37\n\n\n\"Germany\"\n4.31\n\n\n\"UK\"\n3.12\n\n\n\"India\"\n3.04\n\n\n\"France\"\n2.93\n\n\n\"Italy\"\n2.1\n\n\n\"Canada\"\n1.88\n\n\n\"Korea\"\n1.8"
  },
  {
    "objectID": "blog/improve-chart/index.html#creating-a-better-graph",
    "href": "blog/improve-chart/index.html#creating-a-better-graph",
    "title": "How to improve a bad graph with plotly",
    "section": "Creating a better graph",
    "text": "Creating a better graph\nI‚Äôll develop the graph in a step-by-step manner, addressing the problems Olga highlighted in the bad graph until we have a perfect graph that is not only informative but beautiful to look at. I‚Äôll use the graphing library called Plotly.\n\nOut of the box plot\nHere‚Äôs a regular plot without employing any tweaks to make it look better.\n\nimport plotly.graph_objects as go\n\nfig = go.Figure(go.Bar(\n    x=df[\"Country\"],\n    y=df[\"GDP\"]\n))\n\nfig.show(renderer='iframe')\n\n\n\n\nThis regular graph solves the problem of rotated country names, making them easier to read without tilting your head. Another problem that we‚Äôve solve right away is the representation of figures from billion to trillion. However, it‚Äôs still not clear from this graph that the values are in trillion. In the next iteration, we‚Äôll add a title to make this clear.\n\n\nAdd descriptive title\nThe title is an important part of the graph because it tells us what ot focus on. But some titles can be too vague. You wouldn‚Äôt, for instance, use a title like GDP of countries for this graph. You want to use a descriptive title ‚Äì one that tells the audience what to focus on.\n\nfig = go.Figure(go.Bar(\n    x=df[\"Country\"],\n    y=df[\"GDP\"]\n))\n\nfig.update_layout(\n    title=\"&lt;b&gt;Countries with the highest nominal GDP&lt;/b&gt;&lt;br&gt;&lt;sup&gt;&lt;b&gt;(in US $trillion, 2021)&lt;/b&gt;&lt;/sup&gt;\",\n    title_font=dict(size=20),\n)\n\nfig.show(renderer='iframe')\n\n\n\n\n\n\nMake GDP values explicit\nIt‚Äôs very difficult to know the exact GDP values of the countries by looking at the y-axis. To solve this problem, we‚Äôll insert the GDP value for each country on top of their respective bar. Additionally, I‚Äôll hide the values on the y-axis since they won‚Äôt be needed anymore.\n\nfig = go.Figure(go.Bar(\n    x=df[\"Country\"],\n    y=df[\"GDP\"],\n    text=df['GDP'],\n    textposition='outside'\n))\n\nfig.update_layout(\n    title=\"&lt;b&gt;Countries with the highest nominal GDP&lt;/b&gt;&lt;br&gt;&lt;sup&gt;&lt;b&gt;(in US $trillion, 2021)&lt;/b&gt;&lt;/sup&gt;\",\n    title_font=dict(size=20),\n    yaxis=dict(visible=False),\n)\n\nfig.show(renderer='iframe')\n\n\n\n\n\n\nRemove grid lines\nBecause we have removed the values on the y-axis, it‚Äôs pointless to have the horizontal grid lines. We‚Äôll remove them and we‚Äôll also change the background color of the graph.\n\nfig = go.Figure(go.Bar(\n    x=df[\"Country\"],\n    y=df[\"GDP\"],\n    text=df['GDP'],\n    textposition='outside'\n))\n\nfig.update_layout(\n    title=\"&lt;b&gt;Countries with the highest nominal GDP&lt;/b&gt;&lt;br&gt;&lt;sup&gt;&lt;b&gt;(in US $trillion, 2021)&lt;/b&gt;&lt;/sup&gt;\",\n    title_font=dict(size=20),\n    yaxis=dict(showgrid=False, visible=False),\n    plot_bgcolor='#FFE4B5',\n    paper_bgcolor='#FFE4B5',\n)\n\nfig.show(renderer='iframe')\n\n\n\n\n\n\nInclude source and add padding\nFinally, let‚Äôs include the source at the bottom of the graph. We‚Äôll also add padding (spacing) at the top and bottom of the graph so that the title text and the source text are not too close to the edge of the graph.\n\nimport plotly.graph_objects as go\n\nfig = go.Figure(data=[\n    go.Bar(\n        x=df['Country'], \n        y=df['GDP'], \n        marker_color='#1E90FF',\n        text=df['GDP'],\n        textposition='outside'\n    )\n])\n\nfig.update_layout(\n    title=\"&lt;b&gt;Countries with the highest nominal GDP&lt;/b&gt;&lt;br&gt;&lt;sup&gt;&lt;b&gt;(in US $trillion, 2021)&lt;/b&gt;&lt;/sup&gt;\",\n    title_font=dict(size=20),\n    plot_bgcolor='#FFE4B5',\n    paper_bgcolor='#FFE4B5',\n    yaxis=dict(showgrid=False, visible=False),\n    margin=dict(t=70, b=100)\n)\n\nfig.add_layout_image(\n    dict(\n        source=f\"data:image/png;base64,{encoded_image}\",\n        xref=\"paper\",\n        yref=\"paper\",\n        x=0.98,\n        y=-0.26,\n        xanchor=\"right\",\n        yanchor=\"bottom\",\n        sizex=0.22,\n        sizey=0.22,\n        layer=\"above\"\n    )\n)\n\nfig.add_annotation(\n    dict(\n        text=\"&lt;b&gt;Source&lt;/b&gt;: IMF, April 2021\",\n        x=0.01,  # x position (0 means far left)\n        y=-0.26,  # y position (adjust as necessary)\n        xref=\"paper\",\n        yref=\"paper\",\n        showarrow=False,  # No arrow\n        font=dict(\n            size=11,  # Font size\n            color='grey'  # Font color\n        ),\n        align=\"left\"\n    ),\n)\n\nfig.show(renderer='iframe')\n\n\n\n\nNotice that I‚Äôve added the logo for our data consulting company. Contact us if you need any services regarding your data."
  },
  {
    "objectID": "blog/job-interview-details/index.html",
    "href": "blog/job-interview-details/index.html",
    "title": "How overlooking a small detail on a job interview can disqualify you for a position",
    "section": "",
    "text": "A month ago, I interviewed a candidate for a junior data analyst position. Given how difficult it is to land an interview in the data field due to stiff competition, I expected the candidate to meet the basic qualifications, such as:\n\nShowing up on time.\n\nDressing appropriately.\n\nI‚Äôm sure you can think of a few others. I call these the low-hanging-fruit qualifications. Of course, you won‚Äôt find these qualifications listed in a job posting. In other words, your interviewers expect you to automatically meet these qualifications because they require minimal effort. You don‚Äôt need to have a master‚Äôs degree or be a ‚Äú10x programmer‚Äù to meet them. That‚Äôs why they‚Äôre considered low-hanging fruit.\nThe candidate I interviewed showed up on time, but he was dressed in a flannel T-shirt, which put me off. If you can‚Äôt meet the low-hanging-fruit qualifications, how can I trust that you‚Äôll handle the rigorous tasks the job demands? I carried on with the interview out of formality, but the truth is, I had already ruled out the candidate.\nIf you‚Äôre lucky enough to land a job interview, please make sure you meet all the low-hanging-fruit qualifications. You may not enjoy dressing up, but for goodness‚Äô sake, force yourself to do it for the interview. It may seem trivial to you, but it speaks volumes in the interview room, and your interviewers will appreciate it. Don‚Äôt worry about being ‚Äútoo dressed up.‚Äù I‚Äôve learned that:\n\nMany managers would rather coach someone down than coach someone up.\n\nPut another way, it‚Äôs easier to ground someone who‚Äôs overly ambitious than to instill ambition in a lazy person‚Äîit‚Äôs like fighting a losing battle.\nHere‚Äôs my one piece of advice to anyone lucky enough to land a job interview:\n\nIf actions speak louder than words, make sure your actions speak the words you want when you walk into a job interview. Never let anything within your control work against you in that room.\n\nBy the way, this candidate didn‚Äôt get the job‚Äîbut I‚Äôm sure you already guessed that."
  },
  {
    "objectID": "blog/lesson-from-retired-boss/index.html",
    "href": "blog/lesson-from-retired-boss/index.html",
    "title": "The single most important lesson I learned from my retired boss",
    "section": "",
    "text": "The day my boss retired, a small part of me died.\nRewind.\nI thought it was just another first-screening phone call. That‚Äôs what typically comes to mind when you‚Äôve applied to hundreds of jobs and are hoping to hear back from only a handful of them.\n‚ÄúHello,‚Äù I answered the call.\n‚ÄúIs this Joram Mutenge?‚Äù came the voice from the other end.\n‚ÄúThis is he,‚Äù I answered ‚Äì and that was my first introduction to Mike.\nThe call was supposed to last 30 minutes; we ended up talking for an hour. When I hung up, I knew this wasn‚Äôt just another first-screening phone call a job-seeker receives from a recruiter. It felt different for two reasons:\n\nI was speaking directly to the hiring manager.\nHe genuinely wanted to hear my story.\n\nAfter that conversation, I knew I wanted to work for this man ‚Äì and I did. Mike became my boss.\nBut all things, whether good or bad, must come to an end. A few weeks ago, my boss retired. When we had our last lunch together, I told him that I would miss him ‚Äì and I meant it.\nShakespeare was right when he said:\n\nParting is such sweet sorrow.\n\nIt was hard to come to terms with the idea that my beloved boss would no longer be around. Still, it was a good run, and I hope his retirement is just as fun.\nWhat I‚Äôll never forget is something he said to me almost offhandedly, but it‚Äôs a piece of wisdom I‚Äôll carry with me wherever my career takes me:\n\nIn whatever you do at work, remember to ask yourself: ‚ÄúIs this cutting costs or reducing time?‚Äù\n\nThat question is a powerful filter to ensure you‚Äôre working on projects that truly make an impact on the business."
  },
  {
    "objectID": "blog/polars-functions-pros-use/index.html",
    "href": "blog/polars-functions-pros-use/index.html",
    "title": "Ten polars functions that pros use and amateurs don‚Äôt",
    "section": "",
    "text": "Polars is increasingly becoming a popular data analysis library, and my prediction is that more new data scientists and analysts will be starting with Polars rather than Pandas as their tool of choice for manipulating data. After all, the syntax for Polars is easier to learn and harder to forget. That‚Äôs why this tweet couldn‚Äôt be more true.\nHowever, because Polars is new, most of the code out there looks amateurish. Here are 10 functions you should use that will instantly make you look like a pro at Polars."
  },
  {
    "objectID": "blog/polars-functions-pros-use/index.html#scan-csv",
    "href": "blog/polars-functions-pros-use/index.html#scan-csv",
    "title": "Ten polars functions that pros use and amateurs don‚Äôt",
    "section": "1. Scan CSV",
    "text": "1. Scan CSV\nWhen working with large datasets, loading them can take a long time. This is where scan_csv becomes useful. Instead of fully reading the dataset, scan_csv scans its contents, allowing you to quickly preview the file and select only the columns you need. By loading just a subset of the data, you can significantly reduce the loading time. For example, consider a dataset containing information about counties in the USA. import polars as pl\n\npl.read_csv(counties_in_the_usa)\n\n\nshape: (3_143, 7)\n\n\n\nregion\npopulation\ncounty.fips.character\ncounty.name\nstate.name\nstate.fips.character\nstate.abb\n\n\ni64\ni64\ni64\nstr\nstr\ni64\nstr\n\n\n\n\n1001\n54590\n1001\n\"autauga\"\n\"alabama\"\n1\n\"AL\"\n\n\n1003\n183226\n1003\n\"baldwin\"\n\"alabama\"\n1\n\"AL\"\n\n\n1005\n27469\n1005\n\"barbour\"\n\"alabama\"\n1\n\"AL\"\n\n\n1007\n22769\n1007\n\"bibb\"\n\"alabama\"\n1\n\"AL\"\n\n\n1009\n57466\n1009\n\"blount\"\n\"alabama\"\n1\n\"AL\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n56037\n43890\n56037\n\"sweetwater\"\n\"wyoming\"\n56\n\"WY\"\n\n\n56039\n21326\n56039\n\"teton\"\n\"wyoming\"\n56\n\"WY\"\n\n\n56041\n20942\n56041\n\"uinta\"\n\"wyoming\"\n56\n\"WY\"\n\n\n56043\n8425\n56043\n\"washakie\"\n\"wyoming\"\n56\n\"WY\"\n\n\n56045\n7152\n56045\n\"weston\"\n\"wyoming\"\n56\n\"WY\"\n\n\n\n\n\n\n\nSuppose you only want to display the county name and population. Here‚Äôs how you can use scan_csv to achieve that:\n\n(pl.scan_csv(counties_in_the_usa)\n .select('state.name','population')\n .collect()\n )\n\n\nshape: (3_143, 2)\n\n\n\nstate.name\npopulation\n\n\nstr\ni64\n\n\n\n\n\"alabama\"\n54590\n\n\n\"alabama\"\n183226\n\n\n\"alabama\"\n27469\n\n\n\"alabama\"\n22769\n\n\n\"alabama\"\n57466\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"wyoming\"\n43890\n\n\n\"wyoming\"\n21326\n\n\n\"wyoming\"\n20942\n\n\n\"wyoming\"\n8425\n\n\n\"wyoming\"\n7152\n\n\n\n\n\n\n\nNotice that I used collect because scan_csv produces a lazy frame. This means that whenever you use scan_csv, you need to include collect at the end to get your results.\nIf you‚Äôre skeptical that scan_csv is a better approach, let‚Äôs compare the time it takes for scan_csv and read_csv to load the data.\n\n%%timeit\npl.read_csv(counties_in_the_usa)\n\n108 ms ¬± 70.8 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n\n\n\n%%timeit\npl.scan_csv(counties_in_the_usa)\n\n6.41 Œºs ¬± 136 ns per loop (mean ¬± std. dev. of 7 runs, 100,000 loops each)\n\n\nI‚Äôm sure you believe me now."
  },
  {
    "objectID": "blog/polars-functions-pros-use/index.html#sum-horizontal",
    "href": "blog/polars-functions-pros-use/index.html#sum-horizontal",
    "title": "Ten polars functions that pros use and amateurs don‚Äôt",
    "section": "2. Sum horizontal",
    "text": "2. Sum horizontal\nAdding values in a single column is easy. All you need is sum. This is called column-wise and dataframes shine at performing column-wise mathematical operations. However, there comes a time when you need to perform row-wise calculations. This is where sum_horizontal comes in. Unfortunately, most Polars users are not aware of this function. Let‚Äôs say you had this dataframe for Apple stock data.\n\n\n\nshape: (6_953, 7)\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nVolume\nAdj Close\n\n\ndate\nf64\nf64\nf64\nf64\ni64\nf64\n\n\n\n\n2012-03-30\n608.77\n610.56\n597.94\n599.55\n26050900\n599.55\n\n\n2012-03-29\n612.78\n616.56\n607.23\n609.86\n21668300\n609.86\n\n\n2012-03-28\n618.38\n621.45\n610.31\n617.62\n23385200\n617.62\n\n\n2012-03-27\n606.18\n616.28\n606.06\n614.48\n21628200\n614.48\n\n\n2012-03-26\n599.79\n607.15\n595.26\n606.98\n21259900\n606.98\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n1984-09-13\n27.5\n27.62\n27.5\n27.5\n7429600\n3.14\n\n\n1984-09-12\n26.87\n27.0\n26.12\n26.12\n4773600\n2.98\n\n\n1984-09-11\n26.62\n27.37\n26.62\n26.87\n5444000\n3.07\n\n\n1984-09-10\n26.5\n26.62\n25.87\n26.37\n2346400\n3.01\n\n\n1984-09-07\n26.5\n26.87\n26.25\n26.5\n2981600\n3.02\n\n\n\n\n\n\n\nLets also say that you wanted to create a new column called sum_OHLC adds the values in every row for Open, High, Low, Close columns. Most amateur Polars users would write the following code:\n\n(apple_stock\n .with_columns(sum_OHLC=pl.col('Open') + pl.col('High') + pl.col('Low') + pl.col('Close'))\n )\n\nThe way to write the above code like a pro is using sum_horizontal like this:\n\n(apple_stock\n .with_columns(sum_OHLC=pl.sum_horizontal('Open', 'High', 'Low', 'Close'))\n )\n\n\nshape: (6_953, 8)\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nVolume\nAdj Close\nsum_OHLC\n\n\ndate\nf64\nf64\nf64\nf64\ni64\nf64\nf64\n\n\n\n\n2012-03-30\n608.77\n610.56\n597.94\n599.55\n26050900\n599.55\n2416.82\n\n\n2012-03-29\n612.78\n616.56\n607.23\n609.86\n21668300\n609.86\n2446.43\n\n\n2012-03-28\n618.38\n621.45\n610.31\n617.62\n23385200\n617.62\n2467.76\n\n\n2012-03-27\n606.18\n616.28\n606.06\n614.48\n21628200\n614.48\n2443.0\n\n\n2012-03-26\n599.79\n607.15\n595.26\n606.98\n21259900\n606.98\n2409.18\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n1984-09-13\n27.5\n27.62\n27.5\n27.5\n7429600\n3.14\n110.12\n\n\n1984-09-12\n26.87\n27.0\n26.12\n26.12\n4773600\n2.98\n106.11\n\n\n1984-09-11\n26.62\n27.37\n26.62\n26.87\n5444000\n3.07\n107.48\n\n\n1984-09-10\n26.5\n26.62\n25.87\n26.37\n2346400\n3.01\n105.36\n\n\n1984-09-07\n26.5\n26.87\n26.25\n26.5\n2981600\n3.02\n106.12\n\n\n\n\n\n\n\nYou not only write shorter code with sum_horizontal but you can also use other variations of it like mean_horizontal to get the average values of the four numbers and min_horizontal to get the smallest number."
  },
  {
    "objectID": "blog/polars-functions-pros-use/index.html#group-by-dynamic",
    "href": "blog/polars-functions-pros-use/index.html#group-by-dynamic",
    "title": "Ten polars functions that pros use and amateurs don‚Äôt",
    "section": "3. Group by dynamic",
    "text": "3. Group by dynamic\nWhen working with timeseries data, you may need to resample it based on specific time intervals and perform aggregations. Polars provides a convenient function, group_by_dynamic, to handle such tasks efficiently. For example, if you want to calculate the average Close values for Apple stock for each quarter, you can do it as follows:\n\n(apple_stock\n .sort('Date')\n .group_by_dynamic('Date', every='1q')\n .agg(pl.mean('Close'))\n )\n\n\nshape: (111, 2)\n\n\n\nDate\nClose\n\n\ndate\nf64\n\n\n\n\n1984-07-01\n26.73875\n\n\n1984-10-01\n25.288594\n\n\n1985-01-01\n26.690968\n\n\n1985-04-01\n19.212063\n\n\n1985-07-01\n16.015937\n\n\n‚Ä¶\n‚Ä¶\n\n\n2011-01-01\n345.683226\n\n\n2011-04-01\n337.612381\n\n\n2011-07-01\n380.510312\n\n\n2011-10-01\n391.658571\n\n\n2012-01-01\n503.679839\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBefore using group_by_dynamic you must sort the the data on the Date column.\nEven though my data was already sorted, I still used sort to make that explicit.\n\n\nThe beauty of group_by_dynamic is that it can handle highly granular time intervals. Using the every parameter, you can specify intervals such as \"17d\" to resample every 17 days, \"2w\" for 2 weeks, or even \"3s\" for 3-second intervals if your date values include seconds."
  },
  {
    "objectID": "blog/polars-functions-pros-use/index.html#exclude",
    "href": "blog/polars-functions-pros-use/index.html#exclude",
    "title": "Ten polars functions that pros use and amateurs don‚Äôt",
    "section": "4. Exclude",
    "text": "4. Exclude\nIn most cases, you won‚Äôt want to display all the columns in your final dataframe‚Äîjust the ones you‚Äôre interested in. To achieve this, you need to remove the unnecessary columns. Many beginners might choose to drop the columns they don‚Äôt need, but I strongly recommend using exclude instead.\nWhat‚Äôs the difference between excluding columns and dropping them? Dropping a column requires loading it into memory first, which can be time-consuming, especially with large datasets. On the other hand, excluding a column tells Polars‚Äô query engine to skip loading it entirely. This approach is much faster, as Polars only loads the columns you actually need.\nHere‚Äôs how you can use exclude. Suppose you don‚Äôt want to load the Volume and Adj Close columns from the Apple stock dataset. One option is to explicitly select the columns you want to keep, but that would require typing out the names of all five desired columns. Instead, you can use exclude to specify just the two columns you don‚Äôt want displayed, saving both time and effort.\n\n\n\nshape: (6_953, 5)\n\n\n\nDate\nOpen\nHigh\nLow\nClose\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"2012-03-30\"\n608.77\n610.56\n597.94\n599.55\n\n\n\"2012-03-29\"\n612.78\n616.56\n607.23\n609.86\n\n\n\"2012-03-28\"\n618.38\n621.45\n610.31\n617.62\n\n\n\"2012-03-27\"\n606.18\n616.28\n606.06\n614.48\n\n\n\"2012-03-26\"\n599.79\n607.15\n595.26\n606.98\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"1984-09-13\"\n27.5\n27.62\n27.5\n27.5\n\n\n\"1984-09-12\"\n26.87\n27.0\n26.12\n26.12\n\n\n\"1984-09-11\"\n26.62\n27.37\n26.62\n26.87\n\n\n\"1984-09-10\"\n26.5\n26.62\n25.87\n26.37\n\n\n\"1984-09-07\"\n26.5\n26.87\n26.25\n26.5\n\n\n\n\n\n\n\nNow I‚Äôve loaded into memory only the columns I‚Äôm interested in."
  },
  {
    "objectID": "blog/polars-functions-pros-use/index.html#explode",
    "href": "blog/polars-functions-pros-use/index.html#explode",
    "title": "Ten polars functions that pros use and amateurs don‚Äôt",
    "section": "5. Explode",
    "text": "5. Explode\nImagine you have a dataframe that tracks your weekly grocery purchases, with items listed as comma-separated strings in a single row. If you want to identify the items you buy most frequently, this format poses a challenge. Since Polars is a columnar-based framework, working with such data in its current form can make achieving this goal a bit tricky.\n\ngroceries\n\n\nshape: (4, 2)\n\n\n\nDate\nGroceries\n\n\ndate\nstr\n\n\n\n\n2024-12-02\n\"Milk, Eggs, Corn Flakes, Bacon‚Ä¶\n\n\n2024-12-09\n\"Bread, Butter, Apples, Oranges‚Ä¶\n\n\n2024-12-16\n\"Rice, Beans, Chicken, Shampoo,‚Ä¶\n\n\n2024-12-23\n\"Milk, Eggs, Bananas, Yogurt, S‚Ä¶\n\n\n\n\n\n\nThis is where explode comes in. We‚Äôll create a new column Item that will contain a single item as a value for each row. Here‚Äôs how it works. First we split the data on \", \" (comma and space) to convert the string value in Groceries into a list.\n\n(groceries\n .with_columns(pl.col('Groceries').str.split(', '))\n )\n\n\nshape: (4, 2)\n\n\n\nDate\nGroceries\n\n\ndate\nlist[str]\n\n\n\n\n2024-12-02\n[\"Milk\", \"Eggs\", ‚Ä¶ \"Bread\"]\n\n\n2024-12-09\n[\"Bread\", \"Butter\", ‚Ä¶ \"Bacon\"]\n\n\n2024-12-16\n[\"Rice\", \"Beans\", ‚Ä¶ \"Bacon\"]\n\n\n2024-12-23\n[\"Milk\", \"Eggs\", ‚Ä¶ \"Bread\"]\n\n\n\n\n\n\n\nThen we‚Äôll explode the items in each list into individual items by exploding the Groceries column.\nThis new format makes it easier to determine the most bought items by counting how many times each item appears in Groceries.\n\n(groceries\n .with_columns(pl.col('Groceries').str.split(', '))\n .explode('Groceries')\n )\n\n\nshape: (27, 2)\n\n\n\nDate\nGroceries\n\n\ndate\nstr\n\n\n\n\n2024-12-02\n\"Milk\"\n\n\n2024-12-02\n\"Eggs\"\n\n\n2024-12-02\n\"Corn Flakes\"\n\n\n2024-12-02\n\"Bacon\"\n\n\n2024-12-02\n\"Toothpaste\"\n\n\n‚Ä¶\n‚Ä¶\n\n\n2024-12-23\n\"Yogurt\"\n\n\n2024-12-23\n\"Soap\"\n\n\n2024-12-23\n\"Bacon\"\n\n\n2024-12-23\n\"Apples\"\n\n\n2024-12-23\n\"Bread\""
  },
  {
    "objectID": "blog/polars-functions-pros-use/index.html#top-bottom-k",
    "href": "blog/polars-functions-pros-use/index.html#top-bottom-k",
    "title": "Ten polars functions that pros use and amateurs don‚Äôt",
    "section": "6. Top / Bottom K",
    "text": "6. Top / Bottom K\nKnowing the top 10 or 5 highest values or lowest values in your dataset is a very common operation. Polars has two handy functions that you can use to easily display that with top_k and bottom_k. If you wanted to see the top 5 counties in the USA with the highest population, you can use top_k see those counties.\n\n(pl.read_csv(counties_in_the_usa)\n .select('county.name','population')\n .top_k(5, by='population')\n )\n\n\nshape: (5, 2)\n\n\n\ncounty.name\npopulation\n\n\nstr\ni64\n\n\n\n\n\"los angeles\"\n9840024\n\n\n\"cook\"\n5197677\n\n\n\"harris\"\n4101752\n\n\n\"maricopa\"\n3841819\n\n\n\"san diego\"\n3100500\n\n\n\n\n\n\n\nThe top_k function accepts two parameters: a numerical value specifying the number of rows to display and the column to base the sorting on. For instance, in the example above, we used it to find the top 5 largest counties by population. To find the smallest counties by population, you can simply use the bottom_k function instead."
  },
  {
    "objectID": "blog/polars-functions-pros-use/index.html#sample",
    "href": "blog/polars-functions-pros-use/index.html#sample",
    "title": "Ten polars functions that pros use and amateurs don‚Äôt",
    "section": "7. Sample",
    "text": "7. Sample\nWe live in a world of big data and analyzing large datasets can be time-consuming. A smarter approach is to work with a subset of the data, develop and refine your analysis code through experimentation, and then apply the finalized code to the full dataset. But how can you ensure that the subset you choose represents the entire dataset well? This is where the sample function comes in. It allows you to randomly select a specified number of rows. Additionally, these selected rows change with each execution, ensuring a different selection every time you run the code.\nThe US counties dataset contains over three thousand rows, but we‚Äôre going to use sample to only select a thousand rows.\n\n(pl.read_csv(counties_in_the_usa)\n .sample(1000)\n )\n\n\nshape: (1_000, 7)\n\n\n\nregion\npopulation\ncounty.fips.character\ncounty.name\nstate.name\nstate.fips.character\nstate.abb\n\n\ni64\ni64\ni64\nstr\nstr\ni64\nstr\n\n\n\n\n54049\n56460\n54049\n\"marion\"\n\"west virginia\"\n54\n\"WV\"\n\n\n54059\n26660\n54059\n\"mingo\"\n\"west virginia\"\n54\n\"WV\"\n\n\n29510\n318527\n29510\n\"st. louis\"\n\"missouri\"\n29\n\"MO\"\n\n\n1003\n183226\n1003\n\"baldwin\"\n\"alabama\"\n1\n\"AL\"\n\n\n8003\n15750\n8003\n\"alamosa\"\n\"colorado\"\n8\n\"CO\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n51179\n129446\n51179\n\"stafford\"\n\"virginia\"\n51\n\"VA\"\n\n\n18127\n164386\n18127\n\"porter\"\n\"indiana\"\n18\n\"IN\"\n\n\n28059\n139430\n28059\n\"jackson\"\n\"mississippi\"\n28\n\"MS\"\n\n\n48199\n54497\n48199\n\"hardin\"\n\"texas\"\n48\n\"TX\"\n\n\n8089\n18791\n8089\n\"otero\"\n\"colorado\"\n8\n\"CO\"\n\n\n\n\n\n\n\nThe sample function lets you specify the number of rows you want to display as a numerical value. In our case, we are displaying 1000 rows."
  },
  {
    "objectID": "blog/polars-functions-pros-use/index.html#concat-str",
    "href": "blog/polars-functions-pros-use/index.html#concat-str",
    "title": "Ten polars functions that pros use and amateurs don‚Äôt",
    "section": "8. Concat str",
    "text": "8. Concat str\nThis is short for ‚Äúconcatenate string‚Äù and it allows you to create a single value which is a mixture of values from 2 or more columns that contain string values. Suppose we wanted to have a column NameAbbr that contains the state name and the abbreviation for that state, we can do it by using concat_str.\n\n(pl.read_csv(counties_in_the_usa)\n .select('state.name','state.abb')\n .with_columns(pl.concat_str(['state.name','state.abb'],\n                             separator=', '\n                             ).alias('NameAbbr'))\n )\n\n\nshape: (3_143, 3)\n\n\n\nstate.name\nstate.abb\nNameAbbr\n\n\nstr\nstr\nstr\n\n\n\n\n\"alabama\"\n\"AL\"\n\"alabama, AL\"\n\n\n\"alabama\"\n\"AL\"\n\"alabama, AL\"\n\n\n\"alabama\"\n\"AL\"\n\"alabama, AL\"\n\n\n\"alabama\"\n\"AL\"\n\"alabama, AL\"\n\n\n\"alabama\"\n\"AL\"\n\"alabama, AL\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"wyoming\"\n\"WY\"\n\"wyoming, WY\"\n\n\n\"wyoming\"\n\"WY\"\n\"wyoming, WY\"\n\n\n\"wyoming\"\n\"WY\"\n\"wyoming, WY\"\n\n\n\"wyoming\"\n\"WY\"\n\"wyoming, WY\"\n\n\n\"wyoming\"\n\"WY\"\n\"wyoming, WY\"\n\n\n\n\n\n\n\nIn the concat_str function above, we used two parameters. The first parameter is a list of the columns whose values we want to concatenate, and the second specifies the separator to use when joining those values. In this case, we used \", \" (a comma followed by a space) to produce values like ‚Äúwyoming, WY‚Äù."
  },
  {
    "objectID": "blog/polars-functions-pros-use/index.html#format",
    "href": "blog/polars-functions-pros-use/index.html#format",
    "title": "Ten polars functions that pros use and amateurs don‚Äôt",
    "section": "9. Format",
    "text": "9. Format\nWhen presenting data to someone, especially in printed form, you may want to add extra details to make the numbers more understandable. For instance, if you‚Äôre sending the quarterly average Close values you calculated earlier to your boss in the UK and want to add a currency symbol to avoid confusion, you could use format to include the dollar symbol. This way, your boss will always know that your analysis was done in dollars.\n\n(apple_stock\n .sort('Date')\n .group_by_dynamic('Date', every='1q')\n .agg(pl.mean('Close').round(2))\n .with_columns(pl.format(\"${}\", pl.col('Close')).alias('Close'))\n )\n\n\nshape: (111, 2)\n\n\n\nDate\nClose\n\n\ndate\nstr\n\n\n\n\n1984-07-01\n\"$26.74\"\n\n\n1984-10-01\n\"$25.29\"\n\n\n1985-01-01\n\"$26.69\"\n\n\n1985-04-01\n\"$19.21\"\n\n\n1985-07-01\n\"$16.02\"\n\n\n‚Ä¶\n‚Ä¶\n\n\n2011-01-01\n\"$345.68\"\n\n\n2011-04-01\n\"$337.61\"\n\n\n2011-07-01\n\"$380.51\"\n\n\n2011-10-01\n\"$391.66\"\n\n\n2012-01-01\n\"$503.68\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI‚Äôve used round before applying the currency formatting to ensure the figures are rounded to 2 decimal places."
  },
  {
    "objectID": "blog/polars-functions-pros-use/index.html#config",
    "href": "blog/polars-functions-pros-use/index.html#config",
    "title": "Ten polars functions that pros use and amateurs don‚Äôt",
    "section": "10. Config",
    "text": "10. Config\nThe default way of displaying polars dataframes is good, but sometimes you may want to change it up a bit like increase the number of rows displayed or increasing the size of a row to see all the values in that row. The function Config allows you to do just that. Below is the dataframe of groceries. Currently we cannot see all the values contained in each row as indicated by the ellipsis (‚Ä¶).\n\ngroceries\n\n\nshape: (4, 2)\n\n\n\nDate\nGroceries\n\n\ndate\nstr\n\n\n\n\n2024-12-02\n\"Milk, Eggs, Corn Flakes, Bacon‚Ä¶\n\n\n2024-12-09\n\"Bread, Butter, Apples, Oranges‚Ä¶\n\n\n2024-12-16\n\"Rice, Beans, Chicken, Shampoo,‚Ä¶\n\n\n2024-12-23\n\"Milk, Eggs, Bananas, Yogurt, S‚Ä¶\n\n\n\n\n\n\n\nNow Let‚Äôs use Config to increase the size of Groceries.\n\npl.Config(set_fmt_str_lengths=100)\n\ngroceries\n\n\nshape: (4, 2)\n\n\n\nDate\nGroceries\n\n\ndate\nstr\n\n\n\n\n2024-12-02\n\"Milk, Eggs, Corn Flakes, Bacon, Toothpaste, Bread\"\n\n\n2024-12-09\n\"Bread, Butter, Apples, Oranges, Cheese, Bacon\"\n\n\n2024-12-16\n\"Rice, Beans, Chicken, Shampoo, Coffee, Eggs, Bacon\"\n\n\n2024-12-23\n\"Milk, Eggs, Bananas, Yogurt, Soap, Bacon, Apples, Bread\"\n\n\n\n\n\n\n\nThe example above demonstrates a global setting, meaning that the next time a dataframe is displayed, it will apply this size to any column whose values don‚Äôt fit within the default size. This can be frustrating, especially when working with multiple dataframes. To avoid this, you can apply the Config setting to display only one dataframe with the desired settings.\nLet‚Äôs display a dataframe showing the county name and population. I‚Äôll remove the data types from the columns and add commas as thousand separators to make the population figures easier to read. Additionally, I‚Äôll increase the number of rows displayed to 20. Here‚Äôs how the dataframe initially looks.\n\nusa_counties = (pl.read_csv(counties_in_the_usa)\n .select('county.name','population')\n )\nusa_counties\n\n\nshape: (3_143, 2)\n\n\n\ncounty.name\npopulation\n\n\nstr\ni64\n\n\n\n\n\"autauga\"\n54590\n\n\n\"baldwin\"\n183226\n\n\n\"barbour\"\n27469\n\n\n\"bibb\"\n22769\n\n\n\"blount\"\n57466\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"sweetwater\"\n43890\n\n\n\"teton\"\n21326\n\n\n\"uinta\"\n20942\n\n\n\"washakie\"\n8425\n\n\n\"weston\"\n7152\n\n\n\n\n\n\n\nAnd here‚Äôs the display with the formatting in place.\n\nwith pl.Config(set_tbl_rows=20,\n               set_tbl_hide_column_data_types=True,\n               set_thousands_separator=True\n               ):\n    display(usa_counties)\n\n\nshape: (3_143, 2)\n\n\n\ncounty.name\npopulation\n\n\n\n\n\"autauga\"\n54,590\n\n\n\"baldwin\"\n183,226\n\n\n\"barbour\"\n27,469\n\n\n\"bibb\"\n22,769\n\n\n\"blount\"\n57,466\n\n\n\"bullock\"\n10,779\n\n\n\"butler\"\n20,730\n\n\n\"calhoun\"\n117,834\n\n\n\"chambers\"\n34,228\n\n\n\"cherokee\"\n25,917\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"niobrara\"\n2,478\n\n\n\"park\"\n28,203\n\n\n\"platte\"\n8,677\n\n\n\"sheridan\"\n29,097\n\n\n\"sublette\"\n10,065\n\n\n\"sweetwater\"\n43,890\n\n\n\"teton\"\n21,326\n\n\n\"uinta\"\n20,942\n\n\n\"washakie\"\n8,425\n\n\n\"weston\"\n7,152\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe settings applied by Config are only for display purposes. When you export the data to Excel or another format, these configurations, such as comma separators in numbers, will be lost.\n\n\nCheck out this Polars course to learn this powerful Python library for data analysis."
  },
  {
    "objectID": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html",
    "href": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html",
    "title": "Tranforming timeseries data with group by and group by dynamic in polars",
    "section": "",
    "text": "a brand of yogurt\nPolars has become my go-to library for data analysis. Each client project brings new insights into the powerful functionality Polars offers. Recently, I worked on a project for a supermarket that required processing data related to yogurt stock quantities."
  },
  {
    "objectID": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-problem",
    "href": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-problem",
    "title": "Tranforming timeseries data with group by and group by dynamic in polars",
    "section": "The problem",
    "text": "The problem\nThe supermarket‚Äôs data included multiple quantity entries for the same yogurt brand within a single month. The goal was to aggregate these quantities into a single value per month and standardize the date to the first day of that month.\nFor example, the data for a yogurt brand like Chobani in February might look like this:\n\nFeb-02-2025 = 30 units\n\nFeb-08-2025 = 20 units\n\nFeb-15-2025 = 50 units\n\nThe desired output for February would aggregate these values into:\n\nFeb-01-2025 = 100 units\n\nThis aggregation needed to be repeated for every yogurt brand sold by the supermarket."
  },
  {
    "objectID": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-dataset",
    "href": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-dataset",
    "title": "Tranforming timeseries data with group by and group by dynamic in polars",
    "section": "The dataset",
    "text": "The dataset\nTo demonstrate how I solved this problem, I‚Äôll use a representative dataset (not the actual client data).\n\n\n\nshape: (49, 3)\n\n\n\nDate\nYogurt\nQuantity\n\n\ndate\nstr\ni64\n\n\n\n\n2025-02-05\n\"Yoplait\"\n54\n\n\n2025-02-11\n\"Yoplait\"\n54\n\n\n2025-02-14\n\"Yoplait\"\n54\n\n\n2025-02-19\n\"Yoplait\"\n54\n\n\n2025-02-26\n\"Yoplait\"\n54\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-06-03\n\"Wallaby Organic\"\n48\n\n\n2025-06-03\n\"Yoplait\"\n54\n\n\n2025-06-03\n\"Chobani\"\n120\n\n\n2025-06-23\n\"Dannon\"\n12\n\n\n2025-06-24\n\"Chobani\"\n120"
  },
  {
    "objectID": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-solution",
    "href": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#the-solution",
    "title": "Tranforming timeseries data with group by and group by dynamic in polars",
    "section": "The solution",
    "text": "The solution\nI‚Äôll show two approaches to solving this problem. The first method uses group_by, while the second uses group_by_dynamic. Afterward, I‚Äôll verify that both methods produce identical results. More importantly, I‚Äôll compare their performance by using the %%timeit cell magic command to identify the faster solution.\n\nSolution with group by dynamic\nWhen resampling time series data, group_by_dynamic simplifies selecting a specific time period (e.g., weekly, monthly, quarterly) and resampling the data to perform aggregations based on the chosen interval. However, group_by_dynamic does not support grouping by multiple columns. While this limitation may make it unsuitable for addressing the client‚Äôs problem directly, a workaround is available.\nLet‚Äôs process the data for a single yogurt brand, Yoplait to see if the solution is working the way we expect it. Then we‚Äôll repeat the process for all yogurt brands. Here‚Äôs is the unprocessed data for Yoplait yogurt.\n\ndata.filter(pl.col('Yogurt') == \"Yoplait\")\n\n\nshape: (18, 3)\n\n\n\nDate\nYogurt\nQuantity\n\n\ndate\nstr\ni64\n\n\n\n\n2025-02-05\n\"Yoplait\"\n54\n\n\n2025-02-11\n\"Yoplait\"\n54\n\n\n2025-02-14\n\"Yoplait\"\n54\n\n\n2025-02-19\n\"Yoplait\"\n54\n\n\n2025-02-26\n\"Yoplait\"\n54\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-04-30\n\"Yoplait\"\n54\n\n\n2025-05-09\n\"Yoplait\"\n54\n\n\n2025-05-16\n\"Yoplait\"\n54\n\n\n2025-05-26\n\"Yoplait\"\n54\n\n\n2025-06-03\n\"Yoplait\"\n54\n\n\n\n\n\n\n\nHere is the processed data for Yoplait yogurt. The aggregations have been completed, resulting in a reduced number of rows in the dataframe.\n\n(data\n .filter(pl.col('Yogurt') == \"Yoplait\")\n .group_by_dynamic('Date', every='1mo')\n .agg(pl.sum('Quantity'), pl.first('Yogurt'))\n)\n\n\nshape: (5, 3)\n\n\n\nDate\nQuantity\nYogurt\n\n\ndate\ni64\nstr\n\n\n\n\n2025-02-01\n270\n\"Yoplait\"\n\n\n2025-03-01\n270\n\"Yoplait\"\n\n\n2025-04-01\n216\n\"Yoplait\"\n\n\n2025-05-01\n162\n\"Yoplait\"\n\n\n2025-06-01\n54\n\"Yoplait\"\n\n\n\n\n\n\n Notice that I have selected a 1-month time period for the every parameter. However, there is an unresolved issue in our solution: we have multiple dates instead of a single date representing the first day of each month. To address this, I will introduce another parameter, start_by, and set its value to \"window\". This ensures that all dates are converted to the first day of their respective months.\n\n(data\n .filter(pl.col('Yogurt') == \"Yoplait\")\n .group_by_dynamic('Date', every='1mo', start_by='window')\n .agg(pl.sum('Quantity'), pl.first('Yogurt'))\n)\n\n\nshape: (5, 3)\n\n\n\nDate\nQuantity\nYogurt\n\n\ndate\ni64\nstr\n\n\n\n\n2025-02-01\n270\n\"Yoplait\"\n\n\n2025-03-01\n270\n\"Yoplait\"\n\n\n2025-04-01\n216\n\"Yoplait\"\n\n\n2025-05-01\n162\n\"Yoplait\"\n\n\n2025-06-01\n54\n\"Yoplait\"\n\n\n\n\n\n\n Having achieved the desired results for Yoplait yogurt, I can now process the data for the other brands. Instead of processing each brand individually, I will use a for loop to automate the task.\nFirst, I‚Äôll create a list of all the yogurt brands contained in the dataset.\n\nyogurt_list = data['Yogurt'].unique().to_list()\nyogurt_list\n\n['Activia',\n 'Noosa',\n 'Fage',\n 'Wallaby Organic',\n 'Dannon',\n 'Brown Cow',\n \"Siggi's\",\n 'Yoplait',\n 'Chobani',\n 'Stonyfield Organic',\n 'Oikos']\n\n\nAnd now here‚Äôs the code that implements the for loop.\n\ndfs = []\nfor item in yogurt_list:\n    df = (data\n    .filter(pl.col('Yogurt') == item)\n    .group_by_dynamic('Date', every='1mo', start_by='window')\n    .agg(pl.sum('Quantity'), pl.first('Yogurt'))\n    )\n    dfs.append(df)\ndf_1 = pl.concat(dfs)\ndf_1\n\n\nshape: (23, 3)\n\n\n\nDate\nQuantity\nYogurt\n\n\ndate\ni64\nstr\n\n\n\n\n2025-05-01\n504\n\"Activia\"\n\n\n2025-03-01\n12\n\"Noosa\"\n\n\n2025-04-01\n12\n\"Noosa\"\n\n\n2025-05-01\n12\n\"Noosa\"\n\n\n2025-05-01\n24\n\"Fage\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2025-05-01\n408\n\"Chobani\"\n\n\n2025-06-01\n240\n\"Chobani\"\n\n\n2025-04-01\n60\n\"Stonyfield Organic\"\n\n\n2025-04-01\n144\n\"Oikos\"\n\n\n2025-05-01\n144\n\"Oikos\"\n\n\n\n\n\n\n\nTo verify that the above code worked correctly, let display the results for Yoplait yogurt.\n\ndf_1.filter(pl.col('Yogurt') == \"Yoplait\")\n\n\nshape: (5, 3)\n\n\n\nDate\nQuantity\nYogurt\n\n\ndate\ni64\nstr\n\n\n\n\n2025-02-01\n270\n\"Yoplait\"\n\n\n2025-03-01\n270\n\"Yoplait\"\n\n\n2025-04-01\n216\n\"Yoplait\"\n\n\n2025-05-01\n162\n\"Yoplait\"\n\n\n2025-06-01\n54\n\"Yoplait\"\n\n\n\n\n\n\n\nGreat! The results are what we expected.\n\n\nSolution with group by\nFortunately, with group_by, it is possible to aggregate data across multiple columns. This allows me to process the data for all yogurt brands without using a for loop. However, I first need to create a new column, Month, to use as one of the grouping columns in group_by. As before, I will start by processing the data for a single yogurt brand, Yoplait.\n\n(data\n .filter(pl.col('Yogurt') == \"Yoplait\")\n .with_columns(Month=pl.col('Date').dt.month())\n .group_by('Yogurt','Month')\n .agg(pl.sum('Quantity'),\n       pl.first('Date'))\n .drop('Month')\n .with_columns(pl.col(\"Date\").dt.truncate(\"1mo\"))\n )\n\n\nshape: (5, 3)\n\n\n\nYogurt\nQuantity\nDate\n\n\nstr\ni64\ndate\n\n\n\n\n\"Yoplait\"\n216\n2025-04-01\n\n\n\"Yoplait\"\n162\n2025-05-01\n\n\n\"Yoplait\"\n270\n2025-03-01\n\n\n\"Yoplait\"\n270\n2025-02-01\n\n\n\"Yoplait\"\n54\n2025-06-01\n\n\n\n\n\n\n\nNotice that I have used two columns, Yogurt and Month, in group_by to aggregate quantities based on this two-column combination. Since Month has served its purpose, I can drop it as it is no longer needed. However, the date values are not in the expected format. To resolve this issue, I will use truncate and set the value to \"1mo\" to adjust the values in the Date column by one month.\n\n(data\n .filter(pl.col('Yogurt') == \"Yoplait\")\n .with_columns(Month=pl.col('Date').dt.month())\n .group_by('Yogurt','Month')\n .agg(pl.sum('Quantity'),\n       pl.first('Date'))\n .drop('Month')\n .with_columns(pl.col(\"Date\").dt.truncate(\"1mo\"))\n )\n\n\nshape: (5, 3)\n\n\n\nYogurt\nQuantity\nDate\n\n\nstr\ni64\ndate\n\n\n\n\n\"Yoplait\"\n270\n2025-03-01\n\n\n\"Yoplait\"\n162\n2025-05-01\n\n\n\"Yoplait\"\n270\n2025-02-01\n\n\n\"Yoplait\"\n216\n2025-04-01\n\n\n\"Yoplait\"\n54\n2025-06-01\n\n\n\n\n\n\nNow that we have the expected results, all that‚Äôs left to process the data for all yogurt brands is to remove the line of code containing filter.\n\ndf_2 = (data\n .with_columns(Month=pl.col('Date').dt.month())\n .group_by('Yogurt','Month')\n .agg(pl.sum('Quantity'),\n       pl.first('Date'))\n .drop('Month')\n .with_columns(pl.col(\"Date\").dt.truncate(\"1mo\"))\n )\ndf_2\n\n\nshape: (23, 3)\n\n\n\nYogurt\nQuantity\nDate\n\n\nstr\ni64\ndate\n\n\n\n\n\"Siggi's\"\n6\n2025-04-01\n\n\n\"Oikos\"\n144\n2025-05-01\n\n\n\"Yoplait\"\n216\n2025-04-01\n\n\n\"Brown Cow\"\n6\n2025-04-01\n\n\n\"Oikos\"\n144\n2025-04-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Chobani\"\n408\n2025-05-01\n\n\n\"Yoplait\"\n270\n2025-03-01\n\n\n\"Yoplait\"\n54\n2025-06-01\n\n\n\"Yoplait\"\n162\n2025-05-01\n\n\n\"Chobani\"\n444\n2025-03-01"
  },
  {
    "objectID": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#comparing-the-two-methods",
    "href": "blog/tranforming-timeseries-data-with-group-by-and-group-by-dynamic-in-polars/index.html#comparing-the-two-methods",
    "title": "Tranforming timeseries data with group by and group by dynamic in polars",
    "section": "Comparing the two methods",
    "text": "Comparing the two methods\nMultiple factors can be used to determine which code is better, such as ease of writing. However, I will focus on determining which code processes the data faster. Let‚Äôs test which approach performs better.\nWith group by dynamic\n\n%%timeit\n\ndfs = []\nfor item in yogurt_list:\n    df = (data\n    .filter(pl.col('Yogurt') == item)\n    .group_by_dynamic('Date', every='1mo', start_by='window')\n    .agg(pl.sum('Quantity'), pl.first('Yogurt'))\n    )\n    dfs.append(df)\ndf_1 = pl.concat(dfs)\n\n1.74 ms ¬± 24 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nWith group by\n\n%%timeit\n\n(data\n .with_columns(Month=pl.col('Date').dt.month())\n .group_by('Yogurt','Month')\n .agg(pl.sum('Quantity'),\n       pl.first('Date'))\n .drop('Month')\n .with_columns(pl.col(\"Date\").dt.truncate(\"1mo\"))\n )\n\n338 Œºs ¬± 6.97 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nYou can see that group_by processes the data faster, making it the better method. This isn‚Äôt surprising, actually, because it doesn‚Äôt involve using a for loop. It‚Äôs always better to avoid loops when working with dataframes, as this allows your code to be executed in Rust, the language in which Polars was written. When you use for loops, your code is executed in Python, which is slower than Rust.\nCheck out the new Polars for Finance course we published to learn how to process and analyze stock data."
  },
  {
    "objectID": "blog/what-tool-should-you-use-as-a-data-analyst/index.html",
    "href": "blog/what-tool-should-you-use-as-a-data-analyst/index.html",
    "title": "What tool should you use as a data analyst?",
    "section": "",
    "text": "all the tools you‚Äôll want to use\n\n\n\nData analysis is a hot field nowadays. Companies are opening up new data analyst positions, and many people want to become data analysts.\nIt‚Äôs standard knowledge that data analysts work with data. What‚Äôs not standard knowledge is the tools they use. There are just so many data analysis tools out there, and it‚Äôs hard to know what to pick.\nSo what tool should you use in your data analysis job? Tableau? PowerBI?, Jupyer Notebook? Marimo Notebook? And oh, what about git, should you worry about it? What dataframe library should you use, Polars, Pandas, or Ibis? Then there‚Äôs the language wars; R vs Python vs Julia. It‚Äôs just so damn confusing.\nThe answer to what tool you should use may sound unsatisfying, but trust me it‚Äôs the right answer. Are you ready for it?\n\nThe right tool to use for your data analysis work is the tool you know how to use. That‚Äôs it.\n\nIf you can automate tasks with R better than you can automate them with Python, then use R (or vice versa). If wrangling data with Pandas is easier for you than doing it with Polars, use Pandas. If it takes you less time to create visualizations with Tableau than it does PowerBI, use Tableau.\nThe point is that work should not be too difficult to do. When you pull out of your toolbox, always start with the tools you know how to use since you‚Äôll start doing the work immediately rather than figuring out how to use the tool.\nHowever, I‚Äôd encourage you to experiment with new tools every once in a while. You might stumble on a new tool that does the work faster and easier than the old tool you‚Äôre familiar with. It happened to me when I switched from Polars to Pandas.\n\nI came to Polars for the speed and stayed for the syntax.\n\nIt was the best choice, and I‚Äôve never looked back. Had I stuck with Pandas (the dataframe library I knew how to use), I wouldn‚Äôt have experienced how great Polars is."
  }
]